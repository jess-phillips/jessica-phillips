<!DOCTYPE html><html><head>
      <title>STAT-582-notes</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      
        <script type="text/javascript">
          window.MathJax = ({"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"options":{},"loader":{}});
        </script>
        <script type="text/javascript" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
      
      
      
      
      
      <style>
      code[class*=language-],pre[class*=language-]{color:#333;background:0 0;font-family:Consolas,"Liberation Mono",Menlo,Courier,monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.4;-moz-tab-size:8;-o-tab-size:8;tab-size:8;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}pre[class*=language-]{padding:.8em;overflow:auto;border-radius:3px;background:#f5f5f5}:not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em;white-space:normal;background:#f5f5f5}.token.blockquote,.token.comment{color:#969896}.token.cdata{color:#183691}.token.doctype,.token.macro.property,.token.punctuation,.token.variable{color:#333}.token.builtin,.token.important,.token.keyword,.token.operator,.token.rule{color:#a71d5d}.token.attr-value,.token.regex,.token.string,.token.url{color:#183691}.token.atrule,.token.boolean,.token.code,.token.command,.token.constant,.token.entity,.token.number,.token.property,.token.symbol{color:#0086b3}.token.prolog,.token.selector,.token.tag{color:#63a35c}.token.attr-name,.token.class,.token.class-name,.token.function,.token.id,.token.namespace,.token.pseudo-class,.token.pseudo-element,.token.url-reference .token.variable{color:#795da3}.token.entity{cursor:help}.token.title,.token.title .token.punctuation{font-weight:700;color:#1d3e81}.token.list{color:#ed6a43}.token.inserted{background-color:#eaffea;color:#55a532}.token.deleted{background-color:#ffecec;color:#bd2c00}.token.bold{font-weight:700}.token.italic{font-style:italic}.language-json .token.property{color:#183691}.language-markup .token.tag .token.punctuation{color:#333}.language-css .token.function,code.language-css{color:#0086b3}.language-yaml .token.atrule{color:#63a35c}code.language-yaml{color:#183691}.language-ruby .token.function{color:#333}.language-markdown .token.url{color:#795da3}.language-makefile .token.symbol{color:#795da3}.language-makefile .token.variable{color:#183691}.language-makefile .token.builtin{color:#0086b3}.language-bash .token.keyword{color:#0086b3}pre[data-line]{position:relative;padding:1em 0 1em 3em}pre[data-line] .line-highlight-wrapper{position:absolute;top:0;left:0;background-color:transparent;display:block;width:100%}pre[data-line] .line-highlight{position:absolute;left:0;right:0;padding:inherit 0;margin-top:1em;background:hsla(24,20%,50%,.08);background:linear-gradient(to right,hsla(24,20%,50%,.1) 70%,hsla(24,20%,50%,0));pointer-events:none;line-height:inherit;white-space:pre}pre[data-line] .line-highlight:before,pre[data-line] .line-highlight[data-end]:after{content:attr(data-start);position:absolute;top:.4em;left:.6em;min-width:1em;padding:0 .5em;background-color:hsla(24,20%,50%,.4);color:#f4f1ef;font:bold 65%/1.5 sans-serif;text-align:center;vertical-align:.3em;border-radius:999px;text-shadow:none;box-shadow:0 1px #fff}pre[data-line] .line-highlight[data-end]:after{content:attr(data-end);top:auto;bottom:.4em}html body{font-family:'Helvetica Neue',Helvetica,'Segoe UI',Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ol,html body>ul{margin-bottom:16px}html body ol,html body ul{padding-left:2em}html body ol.no-list,html body ul.no-list{padding:0;list-style-type:none}html body ol ol,html body ol ul,html body ul ol,html body ul ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:700;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::after,html body code::before{letter-spacing:-.2em;content:'\00a0'}html body pre>code{padding:0;margin:0;word-break:normal;white-space:pre;background:0 0;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:after,html body pre code:before,html body pre tt:after,html body pre tt:before{content:normal}html body blockquote,html body dl,html body ol,html body p,html body pre,html body ul{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body code,html body pre{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview ul{list-style:disc}.markdown-preview ul ul{list-style:circle}.markdown-preview ul ul ul{list-style:square}.markdown-preview ol{list-style:decimal}.markdown-preview ol ol,.markdown-preview ul ol{list-style-type:lower-roman}.markdown-preview ol ol ol,.markdown-preview ol ul ol,.markdown-preview ul ol ol,.markdown-preview ul ul ol{list-style-type:lower-alpha}.markdown-preview .newpage,.markdown-preview .pagebreak{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center!important}.markdown-preview:not([data-for=preview]) .code-chunk .code-chunk-btn-group{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .status{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .output-div{margin-bottom:16px}.markdown-preview .md-toc{padding:0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link div,.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}.markdown-preview .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0;min-height:100vh}@media screen and (min-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{font-size:14px!important;padding:1em}}@media print{html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc{padding:0 16px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link div,html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% - 300px);padding:2em calc(50% - 457px - 300px / 2);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */
.markdown-preview.markdown-preview {
  font-size: 14px;
}

      </style>
      <!-- The content below will be included at the end of the <head> element. --><script type="text/javascript">
  document.addEventListener("DOMContentLoaded", function () {
    // your code here
  });
</script></head><body for="html-export">
    
    
      <div class="crossnote markdown-preview  ">
      
<h1 id="centerstat-582-advanced-theory-of-statistical-inference-iicenter"><center>STAT 582: Advanced Theory of Statistical Inference II</center> </h1>
<center><b><big>Winter 2024</big></b> <br>
Notes by Alex Luedtke <br> Transcribed by Jess Phillips</center>
<br>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {Augment: {Definitions: {macros: {footnotesize: ['SetSize', .9]}}  // 90% of normal
  });
</script>
<ul>
<li><a href="#stat-582-advanced-theory-of-statistical-inference-ii">STAT 582: Advanced Theory of Statistical Inference II</a>
<ul>
<li><a href="#uchapter-1u-minimaxity"><u>Chapter 1.</u> Minimaxity.</a></li>
<li><a href="#uchapter-2u-kernel-density-estimation"><u>Chapter 2.</u> Kernel Density Estimation.</a></li>
<li><a href="#uchapter-3u-concentration-inequality"><u>Chapter 3.</u> Concentration Inequality.</a></li>
<li><a href="#uchapter-4-part-1u"><u>Chapter 4: Part 1.</u></a></li>
<li><a href="#uchapter-4-part-2u"><u>Chapter 4: Part 2.</u></a></li>
</ul>
</li>
</ul>
<h2 id="uchapter-1u-minimaxity"><u>Chapter 1.</u> Minimaxity. </h2>
<p><big><strong>Chapter 1 Table of Contents</strong></big></p>
<ul>
<li><a href="chapter1\chapter1.md#background">Background</a></li>
<li><a href="chapter1\chapter1.md#minimax-rate-optimality">Minimax rate optimality</a>
<ul>
<li><a href="chapter1\chapter1.md#more-detail">More detail</a></li>
</ul>
</li>
<li><a href="chapter1\chapter1.md#method--1-for-deriving-minimax-lower-bounds">Method # 1 for deriving minimax lower bounds</a></li>
<li><a href="chapter1\chapter1.md#why-we-also-need-other-lower-bounding-techniques">Why we also need other lower bounding techniques</a></li>
</ul>
<p><strong>Other Sections:</strong> <a href="#uchapter-2u-kernel-density-estimation">Chapter 2</a>, <a href="#uchapter-3u-concentration-inequality">Chapter 3</a>, <a href="#uchapter-4-part-1u">Chapter 4 - Part 1</a>, <a href="#uchapter-4-part-2u">Chapter 4 - Part 2</a></p>
<br>
<p></p><h3 id="background">Background </h3>
<ul>
<li>
<p>Suppose we observe data <span class="mathjax-exps">$W\in\mathcal{W}$</span> from a distribution <span class="mathjax-exps">$P$</span> belongings to a model <span class="mathjax-exps">$\mathcal{P}$</span>.</p>
<ul>
<li>We'll be most interested in the special case where <span class="mathjax-exps">$P=Q^n$</span>, so that <span class="mathjax-exps">$W$</span> consists of <span class="mathjax-exps">$n$</span> iid draws from <span class="mathjax-exps">$Q$</span>.</li>
</ul>
</li>
<li>
<p>Based on <span class="mathjax-exps">$w\in W$</span>, we take an action <span class="mathjax-exps">$a$</span> in the action space <span class="mathjax-exps">$\mathcal{A}$</span>.</p>
<ul>
<li>We'll consider a collection of decision rules <span class="mathjax-exps">$T: \mathcal{W} \to \mathcal{A}$</span>.</li>
</ul>
</li>
<li>
<p>The quality of an action is judged by a <strong>loss function</strong> <span class="mathjax-exps">$L: \mathcal{A}\times \mathcal{P} \to \mathbb{R}$</span>.</p>
<ul>
<li>The quality of a decision rule <span class="mathjax-exps">$T$</span> will be judged via the <strong>risk</strong>:<br>
<div class="mathjax-exps">$$R(T,P) = \int L(T(w), P) dP(w).$$</div></li>
</ul>
</li>
</ul>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#0000ff; padding:0.75em; padding-bottom: 0em;">
<p><big><strong>Example 1:</strong> Point estimation.</big></p>
<p>The objective is to estimate <span class="mathjax-exps">$\Psi(P) \in \mathbb{R}$</span>.</p>
<ul>
<li>E.g., <span class="mathjax-exps">$P=Q^n$</span> and <span class="mathjax-exps">$\Psi(P) = \int x \, dQ(x)$</span>.</li>
<li>E.g., <span class="mathjax-exps">$P=Q^n$</span> and <span class="mathjax-exps">$\Psi(P) = E_Q[Y \mid X=x_0]$</span>.</li>
</ul>
<p>If <span class="mathjax-exps">$L(a,P) = \{ a-\Phi(P) \}^2$</span>, then</p>
<p class="math"></p><div class="mathjax-exps">$$R(T,P) = \int \{ T(w) - \Psi(P) \}^2 dP(w) $$</div><p></p><p>is the <strong>mean-squared error (MSE)</strong>.</p>
</div>
<br>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#0000ff; padding:0.75em; padding-bottom: 0em;">
<p><big><strong>Example 2:</strong> Estimating a regression function.</big></p>
<ul>
<li>
<p>We observe <span class="mathjax-exps">$n$</span> iid copies of <span class="mathjax-exps">$(X,Y) \sim Q$</span> and we wish to estimate<br>
</p><div class="mathjax-exps">$$f_Q:x \mapsto E_Q[Y\mid X=x].$$</div><p></p>
</li>
<li>
<p>The action space is a collection of functions mapping from <span class="mathjax-exps">$\mathcal{X}$</span> to <span class="mathjax-exps">$\mathbb{R}$</span>.</p>
</li>
<li>
<p>We might use an <strong>integrated squared error</strong> loss,<br>
</p><div class="mathjax-exps">$$L(a,P) = \int \{ a(x) - f_Q(x) \}^2 d\nu(x).$$</div><p></p>
<p>Here, <span class="mathjax-exps">$\nu$</span> is either some chosen reference measure on <span class="mathjax-exps">$\mathcal{X}$</span> — e.g., Lebesgue — or <span class="mathjax-exps">$\nu$</span> could be the marginal distribution of <span class="mathjax-exps">$X$</span> when <span class="mathjax-exps">$(X,Y) \sim Q$</span>.</p>
<ul>
<li>The corresponding risk is called the <strong>mean integrated squared error (MISE)</strong>.</li>
</ul>
</li>
</ul>
</div>
<br>
<p>Recall from STAT 581 that the performance of a rule <span class="mathjax-exps">$T$</span> can be judged by its <strong>maximal risk</strong>:<br>
</p><div class="mathjax-exps">$$\sup_{P\in\mathcal{P}} R(T,P).$$</div><p></p>
<p>A minimax estimator (decision rule) minimizes the maximal risk.</p>
<p><u><strong>Challenge</strong>:</u> Deriving a minimax estimator is often an <span class="mathjax-exps">$\textcolor{red}{\textbf{intractable problem}}$</span>.<br>
<br></p>
<h3 id="minimax-rate-optimality">Minimax rate optimality </h3>
<p><u>High level (simplified setting):</u> If the minimax risk satisfies the following for some <span class="mathjax-exps">$C_1&gt;0$</span> and <span class="mathjax-exps">$\alpha&gt;0$</span>:<br>
</p><div class="mathjax-exps">$$\underbrace{\inf_{T} \sup_{Q^n} R(T, Q^n)}_{\textcolor{blue}{\text{``minimax risk"}}} = C_1 n^{-\alpha}$$</div><p></p>
<p>then we will call an estimator sequence <span class="mathjax-exps">$(T_n)_{n=1}^{\infty}$</span> minimax rate optimal if<br>
</p><div class="mathjax-exps">$$\sup_{Q^n} R(T_n, Q^n) = C_2 n^{-\alpha}$$</div><p></p>
<p>for some <span class="mathjax-exps">$C_2&gt;0$</span>.</p>
<p><strong>Note:</strong> <span class="mathjax-exps">$TV(P_1, P_2) = 1-\int \min (p_1(x), p_2(x)) d\nu(x)$</span> is the <strong>total variation</strong>, where <span class="mathjax-exps">$p_1 = \frac{dP_1}{d\nu}$</span><br>
<br></p>
<h4 id="more-detail">More detail </h4>
<p>Because we can't typically derive a minimax estimator, we'll instead look to derive estimators with a minimax-optimal rate.</p>
<p>For now, we'll consider cases where <span class="mathjax-exps">$P=Q^n$</span>. For each <span class="mathjax-exps">$n$</span>, let <span class="mathjax-exps">$\mathcal{T}_n$</span> denote the collection of allowable decision rules at sample size <span class="mathjax-exps">$n$</span>. In many problems, the risk is always nonnegative and<br>
</p><div class="mathjax-exps">$$\inf_{T\in\mathcal{T}_n} \sup_{Q\in\mathcal{Q}} R(T, Q^n) \stackrel{n\to\infty}{\longrightarrow} 0,$$</div><p></p>
<p>i.e., the problem is not impossible. We'll focus on these problems here.</p>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#42801f; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Definition.</strong> A sequence of rules <span class="mathjax-exps">$(T_n)_{n=1}^{\infty}$</span> is called <strong>minimax rate optimal</strong> if<br>
</p><div class="mathjax-exps">$$\lim_{n\to\infty} \frac{\displaystyle\small{\inf_{T\in\mathcal{T}_n} \sup_{Q\in\mathcal{Q}} R(T,Q^n)}}{\displaystyle\small{\sup_{Q\in\mathcal{Q}} R(T_n, Q^n)}} &gt; 0. \hspace{3em} \left[ \scriptsize \textcolor{purple}{\frac{n^{-1}}{n^{-1/2}} = n^{-1/2} \to 0} \right] \tag{$\textcolor{orange}{\bigstar}$}$$</div><p></p>
<p>Put another way<br>
</p><div class="mathjax-exps">$$\sup_{Q\in\mathcal{Q}} R(T_n, Q^n) = O \left( \inf_{T\in\mathcal{T}_n} \sup_{Q\in\mathcal{Q}} R(T, Q^n) \right) .$$</div><p></p>
</div>
<br>
<p><u><strong>Problem</strong>:</u> We can't typically derive the numerator or denominator of <span class="mathjax-exps">$(\textcolor{orange}{\bigstar})$</span>!</p>
<p><u><strong>Solution</strong>:</u> Find bounds on them.</p>
<p>Later in thise course, we'll fine <span class="mathjax-exps">$(U_n)_{n=1}^{\infty}$</span> so that<br>
</p><div class="mathjax-exps">$$\sup_{Q\in\mathcal{Q}} R(T_n, Q^n) \leq U_n$$</div><p></p>
<p>for interesting sequences of estimators <span class="mathjax-exps">$(T_n)_{n=1}^{\infty}$</span>.</p>
<p>In this chapter, we'll find <span class="mathjax-exps">$(L_n)_{n=1}^{\infty}$</span> such that<br>
</p><div class="mathjax-exps">$$\inf_{T\in\mathcal{T}_n} \sup_{Q\in\mathcal{Q}} R(T, Q^n) \geq \!\!\!\!\!\!\!\!\!\underbrace{L_n}_{\substack{\text{lower bound} \\ \text{on the minimax} \\ \text{risk}}}\!\!\!\!\!\!\!\!\!.$$</div><p></p>
<p>Once we have these two bounds,<br>
</p><div class="mathjax-exps">$$\frac{\displaystyle\small{\inf_{T\in\mathcal{T}_n} \sup_{Q} R(T,Q^n)}}{\displaystyle\small{\sup_{Q} R(T_n, Q^n)}} \geq \frac{L_n}{U_n}.$$</div><p></p>
<p>Hence, to show <span class="mathjax-exps">$(\bigstar)$</span>, it's enough to show<br>
</p><div class="mathjax-exps">$$\liminf_{n\to\infty} \frac{L_n}{U_n} &gt;0.$$</div><p></p>
<p>In the remainder of this chapter, we'll focus on deriving lower bounds <span class="mathjax-exps">$L$</span> on the minimax risk:<br>
</p><div class="mathjax-exps">$$\inf_{T} \sup_{P} R(T,P) \geq L.$$</div><p></p>
<p><u><strong>Recall from STAT 581</strong>:</u> For any prior <span class="mathjax-exps">$\Pi$</span> on <span class="mathjax-exps">$\mathcal{P}$</span>, the Bayes risk of a decision rule <span class="mathjax-exps">$T$</span> is given by<br>
</p><div class="mathjax-exps">$$r(T, \Pi) = \int R(T,P) \, d\Pi(P).$$</div><p></p>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#aa46e0; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Lemma.</strong> For any decision rule <span class="mathjax-exps">$T$</span>, (<span class="mathjax-exps">$P$</span> taken over all possible distributions and <span class="mathjax-exps">$\Pi$</span> is taken over all possible priors).<br>
</p><div class="mathjax-exps">$$\sup_{P} R(T,P) \geq \sup_{\Pi} r(T,\Pi).$$</div><p></p>
<br>
<p><strong>Proof.</strong> For any <span class="mathjax-exps">$\Pi$</span>, the fact that supremums over <span class="mathjax-exps">$\mathcal{P}$</span> are lower bounded by expectations over distributions supported on <span class="mathjax-exps">$\mathcal{P}$</span> yields<br>
</p><div class="mathjax-exps">$$\underbrace{\sup_{P} R(T, P)}_{\text{maximal risk}} \geq \underbrace{r(T,\Pi)}_{\text{Bayes risk}}.$$</div><p></p>
<p>Taking a supremum over <span class="mathjax-exps">$\Pi$</span> on both sides gives the result. <span class="mathjax-exps">$\hspace{0.5em} \square$</span></p>
</div>
<br>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#43b7d1; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Theorem.</strong> For any decision problem,<br>
</p><div class="mathjax-exps">$$\underbrace{\inf_{T} \sup_{P} R(T,P)}_{\text{``minimax risk"}} \geq \underbrace{\sup_\Pi \inf_T r(T,\Pi)}_{\substack{\text{``Bayes risk under} \\ \text{least favorable prior"}}}.$$</div><p></p>
<br>
<p><strong>Proof.</strong> The max-min inequality yields that<br>
</p><div class="mathjax-exps">$$\inf_T \sup_\Pi r(T,\Pi) \geq \sup_\Pi \inf_T r(T,\Pi).$$</div><p></p>
<p>Combining this with the preceding lemma yields the desired result. <span class="mathjax-exps">$\hspace{0.5em} \square$</span></p>
</div>
<br>
<p>The main consequence of this theorem that we use in this chapter is the following:</p>
<div style="color:#aa46e0">
<ul>
<p></p><div class="mathjax-exps">$$\text{for any prior }\Pi, \inf_T \sup_P R(T, P) \geq \inf_T r(T,\Pi) \tag{1}$$</div><p></p>
</ul>
</div>
<h3 id="method--1-for-deriving-minimax-lower-bounds">Method # 1 for deriving minimax lower bounds </h3>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#43b7d1; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Theorem (Le Cam).</strong> Let <span class="mathjax-exps">$R$</span> be a risk function defined according to some nonnegative loss <span class="mathjax-exps">$L$</span>. For any <span class="mathjax-exps">$P_1, P_2 \in\mathcal{P}$</span>,<br>
</p><div class="mathjax-exps">$$\begin{align*}     \inf_T \sup_P R(T,P) &amp;\geq \frac{1}{2} \textcolor{blue}{d(P_1, P_2)} \textcolor{red}{\Vert p_1 \wedge p_2 \Vert_1} \\          &amp;\geq \frac{1}{4} \textcolor{blue}{d(P_1, P_2)} \exp [-\textcolor{green}{KL(P_1,P_2)}] \end{align*}$$</div><p></p>
<p>where<br>
</p><div class="mathjax-exps">$$\begin{align*}     \textcolor{blue}{d(P_1,P_2)} &amp;= \inf_{a\in\mathcal{A}} [L(a,P_1) + L(a,P_2)], \hspace{7.958em}\text{(discrepancy)} \\ \textcolor{red}{\Vert p_1 \wedge p_2 \Vert_1} &amp;= \int \min \left\{ \frac{dP_1}{d\nu}(w),  \frac{dP_2}{d\nu}(w)\right\}, \hspace{6.3em} \text{(testing affinity)} \\ \textcolor{green}{KL(P_1,P_2)} &amp;= \begin{cases}     \int \log \left[ \frac{dP_1}{dP_2} (w) \right] dP_1(w), &amp; \text{if } P_1 \ll P_2, \\     +\infty, &amp; \text{otherwise} \end{cases} \hspace{2em} \text{(KL divergence)} \end{align*}$$</div><p></p>
</div>
<br>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#000000; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Remark 1.</strong> Understanding the discrepancy.</p>
<p>We'll look at the discrepancy in two special cases.</p>
<ol type="a">
   <li>
<p><u>Point estimation with squared error loss:</u><br>
We want to estimate <span class="mathjax-exps">$\Psi(P)\in\mathbb{R}$</span> and<br>
</p><div class="mathjax-exps">$$L(a,P) = [a - \Psi(P)]^2.$$</div> A little calculus shows that<br>
<div class="mathjax-exps">$$\begin{align*}         d(P_1,P_2) &amp;= \inf_{a\in\mathbb{R}} [L(a,P_1) + L(a,P_2)] \\         &amp;= \frac{1}{2} [\Psi(P_1) - \Psi(P_2)]^2    \end{align*}$$</div><p></p>
   </li>
   <li>
<p><u>Estimating a function with respect to the integrated squared error:</u></p>
<p>Here, <span class="mathjax-exps">$P=Q^n$</span>, the action space is a convex subset of the space functions mapping from <span class="mathjax-exps">$\mathcal{X}$</span> to <span class="mathjax-exps">$\mathbb{R}$</span>, and<br>
</p><div class="mathjax-exps">$$L(a, P) = \int [a(x) - f_Q(x)]^2 d\nu(x). \hspace{2em} (\nu \text{ is a fixed measure})$$</div><p></p>
<p>In this case, you can show that<br>
</p><div class="mathjax-exps">$$d(P_1,P_2) = \frac{1}{2} \int [f_{Q_1}(x) - f_{Q_2}(x)]^2 d\nu(x),$$</div><p></p>
<p>where <span class="mathjax-exps">$P_1 = Q_1^n$</span> and <span class="mathjax-exps">$P_2 = Q_2^n$</span>.</p>
   </li>
</ol>
</div>
<br>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#000000; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Remark 2.</strong> Understanding testing affinity.</p>
<p>Statistically, we can show that, give a draw <span class="mathjax-exps">$W$</span> from <span class="mathjax-exps">$P_j$</span>, <span class="mathjax-exps">$j\in\{ 1,2 \}$</span> unknown, the difficulty of identifying <span class="mathjax-exps">$j$</span> is characterized by <span class="mathjax-exps">$\Vert p_1 \wedge p_2 \Vert_1$</span>.</p>
<p>![drawings of p_1(w) and p_2(w)](/jessica-phillips/img<br>
/IMG_0348.jpg)</p>
<p>Note that<br>
</p><div class="mathjax-exps">$$\textcolor{purple}{ \Vert p_1 \wedge p_2 \Vert_1 = \int \min \{ p_1(w), p_2(w) \} d \nu(w)}.$$</div><p></p>
</div>
<br>
<ul>
<li>
<p>Consider a test <span class="mathjax-exps">$T: \mathcal{W} \to \{ 1,2 \}$</span>.<br>
<br></p>
<p>We'll quantify performance of <span class="mathjax-exps">$T$</span> via misclassification error:<br>
</p><div class="mathjax-exps">$$\mathcal{E}(T) = \underbrace{E_{P_1}[I\{ T(W) = 2 \}]}_{\text{error under } P_1} + \underbrace{E_{P_2}[I\{ T(W) = 1 \}]}_{\text{error under } P_2}.$$</div><p></p>
<p>Observe that, for any <span class="mathjax-exps">$T$</span>,<br>
</p><div class="mathjax-exps">$$\begin{align*}   \mathcal{E}(T) &amp;= \int [p_1(w) I\{ T(w) = 2 \} + p_2(w) I\{ T(w) = 1 \}] d\nu(w) \\       &amp;\geq \int \min \{ p_1(w), p_2(w) \} [\underbrace{I\{ T(w) = 2 \} + I\{ T(w) = 1 \}}_{\textcolor{blue}{=1}}] d\nu(w) \\       &amp;= \Vert p_1 \wedge p_2 \Vert_1. \end{align*}$$</div><p></p>
<p>The lower bound is achieved by<br>
</p><div class="mathjax-exps">$$T^\star(w) = \argmax_{j} p_j(w).$$</div><p></p>
<p><strong>Note:</strong> <span class="mathjax-exps">$\textcolor{blue}{\mathcal{E}(T^\star) = \Vert p_1 \wedge p_2 \Vert_1}$</span>.</p>
</li>
</ul>
<br>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#000000; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Remark 3.</strong> (Not responsible for knowing this in this course.) <span class="mathjax-exps">$\Vert p_1 \wedge p_2 \Vert_1 = 1 - TV(P_1,P_2)$</span>, where<br>
</p><div class="mathjax-exps">$$TV(P_1, P_2) = \sup_A |P_1(A) - P_2(A)|.$$</div><p></p>
</div>
<br><br>
<p><strong>Proof (Le Cam).</strong> Let <span class="mathjax-exps">$\Pi$</span> denote a uniform prior over <span class="mathjax-exps">$\{ P_1, P_2 \}$</span>, that is, <span class="mathjax-exps">$\pi(P_1) = \pi(P_2) = \frac{1}{2}$</span>.</p>
<p>For any rule <span class="mathjax-exps">$T$</span>,<br>
</p><div class="mathjax-exps">$$\begin{align*}     r(T, \Pi) &amp;= \sum_{j=1}^{2} \int L(T(w), P_j) \, p_j(w)\, d\nu(w) \; \pi(P_j) \\         &amp;= \frac{1}{2} \int \left[ \sum_{j=1}^2 L(T(w), P_j) \, p_j(w)\right] d\nu(w) \hspace{2em} (\text{since } \pi(P_j) = \tfrac{1}{2}) \\         &amp;\geq \frac{1}{2} \int \min \{ p_1(w), p_2(w) \} \left[ \sum_{j=1}^{2} L(T(w), P_j) \right] d\nu(w) \\         &amp;\geq \frac{1}{2} \int \min \{ p_1(w), p_2(w) \} \underbrace{\left[ \inf_{a\in\mathcal{A}} \sum_{j=1}^{2} L(a, P_j)\right] }_{\textcolor{blue}{:= d(P_1, P_2)}} d\nu(w) \\         &amp;= \frac{1}{2} d(P_1,P_2) \underbrace{\int \min \{ p_1(w), p_2(w) \} d\nu(w)}_{\textcolor{blue}{:= \Vert p_1 \wedge p_2 \Vert_1}}. \end{align*}$$</div><p></p>
<p>As <span class="mathjax-exps">$T$</span> was arbitrary,<br>
</p><div class="mathjax-exps">$$\inf_{T} r(T,\Pi) \geq \frac{1}{2} d(P_1, P_2) \Vert p_1 \wedge p_2 \Vert_1.$$</div><p></p>
<p>By Remark 1, this yields<br>
</p><div class="mathjax-exps">$$\inf_T \sup_{P} R(T,P) \geq \frac{1}{2} d(P_1,P_2) \Vert p_1 \wedge p_2 \Vert_1.$$</div><p></p>
<p>The second bound of the theorem follows from problem 1 on homework 1.<br>
<br></p>
<h3 id="why-we-also-need-other-lower-bounding-techniques">Why we also need other lower bounding techniques </h3>
<ul>
<li>
<p>In certain cases, Le Cams method will provide a rate-optimal lower bound.</p>
<ul>
<li>This is the case on problem 2 of homework 1, which concerns estimation of a smooth density at a point.<br>
<br></li>
</ul>
</li>
<li>
<p>In other cases, Le Cam's method doesn't give a rate-optimal lower bound.</p>
<ul>
<li>This is the case when the goal is to estimate a smooth regression function, with integrated squared error loss.</li>
</ul>
</li>
</ul>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#43b7d1; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Theorem (Fano).</strong> Let <span class="mathjax-exps">$R$</span> be a risk function defined according to some nonnegative loss <span class="mathjax-exps">$L$</span>. For <span class="mathjax-exps">$N\geq 3$</span> and <span class="mathjax-exps">$P_1, P_2, \dots, P_N \in \mathcal{P}$</span> (the distributions contained in the model), define<br>
</p><div class="mathjax-exps">$$\begin{align*}     \eta &amp;:= \min_{j\neq k} d(P_j, P_k) = \min_{j\neq k} \inf_a [L(a, P_j) + L(a,P_k)], \\     \bar{P} &amp;:= \frac{1}{N} \sum_{j=1}^{N} P_j, \hspace{2em} \textcolor{orange}{(\text{uniform mixture of  $P_1,P_2,\dots,P_n$ })} \tag{$\textcolor{orange}{\bigstar}$}\\     K &amp;:= \max_{j\neq k} \text{KL}(P_j, P_k). \end{align*}$$</div><p></p>
<p>(<strong>Note on <span class="mathjax-exps">$(\textcolor{orange}{\bigstar})$</span></strong> (see lecture notes screenshot))</p>
<p>It holds that<br>
</p><div class="mathjax-exps">$$\begin{align*}     \inf_T \sup_P R(T,P) &amp;\geq \frac{\eta}{2} \left[ 1 - \frac{\log(2) + \frac{1}{N} \sum_{j=1}^{N} \text{KL}(P_j, \bar{P})}{\log(N)} \right] \\         &amp;\geq \frac{\textcolor{purple}{\eta}}{2} \left[ 1 - \frac{\log(2) + \textcolor{blue}{\Kappa}}{\log(\textcolor{red}{N})} \right] \end{align*}$$</div><p></p>
</div>
<br>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#000000; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Remark.</strong> The <u>bound increases</u> when:</p>
<ul>
<li><u><span class="mathjax-exps">$\textcolor{red}{N}$</span> increases</u> and <span class="mathjax-exps">$\textcolor{purple}{\eta}$</span>, <span class="mathjax-exps">$\textcolor{blue}{\Kappa}$</span> remain fixed.</li>
<li><u><span class="mathjax-exps">$\textcolor{blue}{\Kappa}$</span> decreases</u> and <span class="mathjax-exps">$\textcolor{purple}{\eta}$</span>, <span class="mathjax-exps">$\textcolor{red}{N}$</span> remain fixed.</li>
<li><u><span class="mathjax-exps">$\textcolor{purple}{\eta}$</span> increases</u> and <span class="mathjax-exps">$\textcolor{blue}{\Kappa}$</span>, <span class="mathjax-exps">$\textcolor{red}{N}$</span> remain fixed.</li>
</ul>
<p>Moreover, adding a distribution to the set <span class="mathjax-exps">$\{ P_1,\dots,P_N \}$</span> (i.e., increasing <span class="mathjax-exps">$N$</span>) typically decreases <span class="mathjax-exps">$\eta$</span> and increases <span class="mathjax-exps">$\Kappa$</span>.</p>
<br>
<p><strong>Proof.</strong> Let <span class="mathjax-exps">$\Pi$</span> denote a uniform prior over <span class="mathjax-exps">$\{ P_1,\dots,P_N \}$</span>. For any rule <span class="mathjax-exps">$T$</span>, we have that<br>
</p><div class="mathjax-exps">$$\begin{align*}     r(T,\Pi) &amp;= \frac{1}{N} \sum_{j=1}^{N} \int \underbrace{L(T(w), P_j)}_{\textcolor{orange}{``\ell"}} p_j(w) d\nu(w) \\         &amp;\geq \frac{\eta}{2N} \sum_{j=1}^{n} \int I \left\{ L(T(w), P_j) \geq \frac{\eta}{2} \right\} p_j(w) \, d\nu(w), \tag{2} \end{align*}$$</div><p></p>
<p>where we've used that <span class="mathjax-exps">$\ell\geq \frac{\eta}{2} I\left\{ \ell \geq \frac{\eta}{2} \right\}$</span> for all <span class="mathjax-exps">$\ell\geq 0$</span>.</p>
<p>Continuing <span class="mathjax-exps">$(2)$</span>, we see<br>
</p><div class="mathjax-exps">$$r(T,\Pi) \geq \frac{\eta}{2} \left[ 1 - \frac{1}{N} \int \underbrace{\sum_{j=1}^{N} I \left\{ L(T(w), P_j)&lt; \frac{\eta}{2} \right\} p_j(w)}_{\substack{\textcolor{purple}{\text{We'll now upper bound this}} \\ \textcolor{purple}{\text{quantity by } \max_j p_j(w)}}} d\nu(w) \right].$$</div><p></p>
<p>For each <span class="mathjax-exps">$w$</span>,<br>
</p><div class="mathjax-exps">$$\begin{align*}     \sum_{j=1}^{N} I \left\{ L(T(w), P_j) &lt; \frac{\eta}{2} \right\} p_j(w) &amp;\leq \left[ \max_j p_j(w) \right] \sum_{j=1}^{N} I \left\{ L(T(w), P_j) &lt; \frac{\eta}{2} \right\} \\         &amp;\leq \left[ \max_j p_j(w) \right] \underbrace{\sup_a \sum_{j=1}^{N} I \left\{ L(a, P_j) &lt; \frac{\eta}{2} \right\}}_{\textcolor{red}{\text{we'll show this is } \leq 1}}. \end{align*}$$</div><p></p>
<p>To show<br>
</p><div class="mathjax-exps">$$\sup_a \sum_{j=1}^{N} I \left\{ L(a, P_j) &lt; \frac{\eta}{2} \right\} \leq 1, \tag{$\bigstar$}$$</div><p></p>
<p>it will be useful to define <span class="mathjax-exps">$\eta_a = \min_{j\neq k} [L(a,P_j) + L(a,P_k)]$</span>.</p>
<p><strong>Note:</strong> <span class="mathjax-exps">$\eta = \inf_a \eta_a$</span>, and so <span class="mathjax-exps">$\eta\leq \eta_a$</span> for all <span class="mathjax-exps">$a$</span>.</p>
<p>Fix an action <span class="mathjax-exps">$a$</span>. We'll show that<br>
</p><div class="mathjax-exps">$$\sum_{j=1}^{N} I \left\{ L(a,P_j) &lt; \frac{\eta}{2} \right\} \leq 1$$</div><p></p>
<p>![number lines](/jessica-phillips/img<br>
/IMG_0357.jpg)</p>
<br>
<p>Also, because <span class="mathjax-exps">$\frac{\eta}{2} \leq \frac{\eta_a}{2}$</span>, we have<br>
</p><div class="mathjax-exps">$$\sum_{j=1}^{N} I \left\{ L(a,P_j) &lt; \frac{\eta}{2} \right\} \leq \sum_{j=1}^{N} I \left\{ L(a, P_j) &lt; \frac{\eta_a}{2} \right\} \leq 1.$$</div><p></p>
<p>Taking a supremum over <span class="mathjax-exps">$a$</span> gives <span class="mathjax-exps">$(\bigstar)$</span>. Plugging this into <span class="mathjax-exps">$(3)$</span> yields<br>
</p><div class="mathjax-exps">$$r(T,\Pi) \geq \frac{\eta}{2} \left[ 1 - \frac{1}{N} \int \left[ \max_j p_j(w) \right] d\nu(w) \right].$$</div><p></p>
<p>On slide 23 of the lecture notes, we show<br>
</p><div class="mathjax-exps">$$\begin{align*}     \max_j p_j(w) \leq&amp; \frac{N \bar{p}(w) \log(2) + \sum_{j=1}^{N} p_j(w) \log \frac{p_j(w)}{\bar{p}(w)}}{\log(N)} \\         &amp;\implies \int \max_j p_j(w) d\nu(w) \leq \frac{N\log(2) + \sum_{j=1}^{N} \text{KL}(P_j, \bar{P})}{\log(N)} \\         &amp;\implies \frac{1}{N} \int \max_j p_j(w) d\nu(w) \leq \frac{\log(2) + \frac{1}{N} \sum_{j=1}^{N} \text{KL}(P_j, \bar{P})}{\log(N)}. \end{align*}$$</div><p></p>
<p>Plugging this into <span class="mathjax-exps">$(4)$</span> gives<br>
</p><div class="mathjax-exps">$$r(T, \Pi) \geq \frac{\eta}{2} \left[ 1 - \frac{\log(2) + \frac{1}{N} \sum_{j=1}^{N} \text{KL}(P_j,\bar{P})}{\log(N)} \right].$$</div><p></p>
<p>Since <span class="mathjax-exps">$\sup_p R(T, P) \geq r(T,\Pi)$</span>, this yields the first bound of the theorem statement.</p>
<p>The second bound follows from the fact that <span class="mathjax-exps">$\text{KL}(P_j, \cdot)$</span> is convex for all <span class="mathjax-exps">$j$</span>. Indeed,<br>
</p><div class="mathjax-exps">$$\begin{align*}     \text{KL}(P_j, \bar{P}) &amp;= \text{KL} \left( P_j, \frac{1}{N} \sum_{i=1}^{N} P_i \right) \\         &amp;\leq \frac{1}{N} \sum_{i=1}^{N} \text{KL}(P_j, P_i) \\         &amp;\leq \max_{i\neq j} \text{KL}(P_j, P_i). \end{align*}$$</div><p></p>
</div>
<br>
<hr>
<p><big><strong>Example:</strong> Minimax lower bound for estimating a smooth regression function.</big></p>
<ul>
<li>
<p>We observe <span class="mathjax-exps">$(X_1,Y_1), \dots, (X_n, Y_n) \stackrel{iid}{\sim} Q \in \mathcal{Q}$</span>.<br>
<br></p>
</li>
<li>
<p>For each <span class="mathjax-exps">$Q$</span>, the marginal distribution of <span class="mathjax-exps">$X$</span> is <span class="mathjax-exps">$U[0,1]$</span>.<br>
<br></p>
</li>
<li>
<p>Moreover,<br>
</p><div class="mathjax-exps">$$Y\mid X=x \sim N(f_Q(x), 1)$$</div><p></p>
<p>where <span class="mathjax-exps">$f_Q$</span> is smooth — in particular, <span class="mathjax-exps">$f_Q \in \mathcal{F}(\beta,L)$</span> with <span class="mathjax-exps">$\mathcal{F}(\beta,L)$</span> denoting the Holder class:<br>
</p><div class="mathjax-exps">$$\mathcal{F}(\beta,L) := \left\{ f: f \text{ is } \ell \text{-times differentiable and } |f^{\ell}(x_1) - f^{\ell}(x_2) | \leq |x_1-x_2|^{\beta-\ell} \text{ for all } x_1, x_2 \in [0,1] \right\}$$</div><p></p>
<p>where <span class="mathjax-exps">$\ell$</span> is the greatest integer strictly smaller than <span class="mathjax-exps">$\beta$</span>.</p>
</li>
</ul>
<p><strong>Note:</strong> A sufficient condition for a function <span class="mathjax-exps">$f$</span> to belong to <span class="mathjax-exps">$\mathcal{F}(\beta,L)$</span> is that <span class="mathjax-exps">$f$</span> be <span class="mathjax-exps">$(\ell+1)$</span>-times differentiable and<br>
</p><div class="mathjax-exps">$$\sup_x \left| f^{(\ell+1)}(x) \right| \leq L.$$</div><p></p>
<p>Our objective is to estimate <span class="mathjax-exps">$f_Q(x) = E_Q[Y\mid X=x]$</span>. Performance will be quantified via the integrated squared error:<br>
</p><div class="mathjax-exps">$$L(a, P) = L(a, Q^n) = \int_{0}^{1} [a(x) - f_Q(x)]^2 dx.$$</div><p></p>
<p>We will derive a minimax lower bound for this regression function via Fano's method.</p>
<ul>
<li>to do this, we must select a finite set <span class="mathjax-exps">$\mathcal{P} := \{ P_1,\dots,P_N \} \subseteq \mathcal{P}$</span>.</li>
<li>We'll let <span class="mathjax-exps">$P_j = Q_j^n$</span> for <span class="mathjax-exps">$Q_j\in \mathcal{Q}$</span>.</li>
</ul>
<p>For any <span class="mathjax-exps">$\omega \in \{ 0,1 \}^m$</span>, we define<br>
</p><div class="mathjax-exps">$$f_\omega(x) = \sum_{j=1}^{m} \omega_j \phi_j(x),$$</div><p></p>
<p>where, for a fixed constant <span class="mathjax-exps">$h&gt;0$</span>, we have <span class="mathjax-exps">$m \in \left[ 8, \frac{1}{h} - 1 \right]$</span> and<br>
</p><div class="mathjax-exps">$$\phi_j(x) = Lh^\beta K \left( \frac{x-\frac{j}{m+1}}{h} \right).$$</div><p></p>
<p>Here, for sufficiently small <span class="mathjax-exps">$a&gt;0$</span>, <span class="mathjax-exps">$K$</span> is defined as<br>
</p><div class="mathjax-exps">$$K(x) = a \exp \left( - \frac{1}{1-4x^2} \right) I \left\{ |x| &lt; \frac{1}{2} \right\}.$$</div><p></p>
<p>Below we can see a plot of the function <span class="mathjax-exps">$K$</span>.<br>
![plot of K](/jessica-phillips/img<br>
/IMG_0358.jpg)</p>
<p>The quantity <span class="mathjax-exps">$a$</span> is chosen so that <span class="mathjax-exps">$f_\omega \in \mathcal{F}(\beta,L)$</span> for all <span class="mathjax-exps">$\omega \in \{ 0,1 \}^m$</span></p>
<center>
<p>![](/jessica-phillips/img<br>
/IMG_0359.jpg)</p>
<p>![](/jessica-phillips/img<br>
/IMG_0360.jpg)</p>
</center>
<p>If, for some <span class="mathjax-exps">$\widetilde{\Omega} \subseteq \{ 0,1 \}^m$</span>, we applied Fano's method based on <span class="mathjax-exps">$\{ P_\omega = Q_\omega^n: \omega \in \widetilde{\Omega} \}$</span>, where <span class="mathjax-exps">$Q_\omega$</span> is the distribution with regression function <span class="mathjax-exps">$f_\omega$</span>,<br>
</p><div class="mathjax-exps">$$\textcolor{red}{\inf_T \sup_{p} \mathcal{R}(T,P) \geq \frac{\min_{\omega\neq \nu} d(P_\omega, P_\nu)}{2} \left( 1 - \frac{\log(2) + \max_{\omega\neq \nu} \text{KL}(P_\omega, P_\nu)}{\log \left| \widetilde{\Omega} \right|} \right)}.$$</div><p></p>
<p>From earlier in this chapter<br>
</p><div class="mathjax-exps">$$\textcolor{blue}{d(P_\omega, P_\nu) = \frac{1}{2} \int [f_\omega(x) - f_\nu(x)]^2 dx}.$$</div><p></p>
<p>Also, from slide 35 in the notes,<br>
</p><div class="mathjax-exps">$$\textcolor{blue}{\text{KL}(P_\omega, P_\nu) = \frac{n}{2} \int [f_\omega(x) - f_\nu(x)]^2 dx}.$$</div><p></p>
<p>Since none of the bump functions overlap, <span class="mathjax-exps">$\phi_j(x) \phi_k(x) = 0$</span> for all <span class="mathjax-exps">$j\neq k$</span>, and so<br>
</p><div class="mathjax-exps">$$\begin{align*}     \int [f_\omega(x) - f_\nu(x)]^2 dx &amp;= \int_{0}^{1} \left[ \sum_{j=1}^{m} (\omega_j - \nu_j) \phi_j(x) \right]^2 dx \\         &amp;= \sum_{j=1}^{m} (\omega_j - \nu_j) \int \phi_j(x)^2 dx \\         &amp;= \sum_{j=1}^{m} (\omega_j - \nu_j)^2 L^2 h^{2\beta+1} \underbrace{\int K(u)^2 du}_{c_2} \tag{u-substitution} \\         &amp;= c_2 L^2 h^{2\beta+1} H(\omega, \nu), \end{align*}$$</div><p></p>
<p>where <span class="mathjax-exps">$H(\omega,\nu) := \sum_{j=1}^{n} (\omega_j - \nu_j)^2$</span> denotes the Hamming distance between <span class="mathjax-exps">$\omega$</span> and <span class="mathjax-exps">$\nu$</span>.</p>
<p>Plugging the above into the preceding two colored expressions and <span class="mathjax-exps">$c_3 := \frac{c_2 L^2}{2}$</span>, we find that<br>
</p><div class="mathjax-exps">$$d(P_\omega, P_\nu) = c_3 h^{2\beta+1} H(\omega,\nu)$$</div><p></p>
<p>and<br>
</p><div class="mathjax-exps">$$\text{KL}(P_\omega, P_\nu) = c_3 n h^{2\beta+1} H(\omega, \nu).$$</div><p></p>
<p>Also, since <span class="mathjax-exps">$H(\omega, \nu) \leq m$</span>, we have that<br>
</p><div class="mathjax-exps">$$\textcolor{blue}{\max_{\omega\neq \nu} \text{KL}(P_\omega, P_\nu) \leq c_3 n h^{2\beta+1} m}.$$</div><p></p>
<p>Plugging the above into <span class="mathjax-exps">$\textcolor{red}{\text{Fano's lower bound}}$</span>,<br>
</p><div class="mathjax-exps">$$\inf_T \sup_p R(T,P) \geq \frac{c_3 h^{2\beta+1}}{2} \min_{\omega\neq \nu} H(\omega,\nu) \cdot \left[ 1 - \frac{\log(2) + c_3 n h^{2\beta+1} m}{ \log \left| \widetilde{\Omega} \right|} \right].$$</div><p></p>
<p>In what follows, we'll consider two choices of <span class="mathjax-exps">$\widetilde{\Omega} \subseteq \{ 0,1 \}^m$</span>.</p>
<ul>
<li>
<p><strong>Choice 1:</strong> <span class="mathjax-exps">$\widetilde{\Omega} = \{ 0,1 \}^m$</span>. In this case,<br>
</p><div class="mathjax-exps">$$\begin{align*}   \min_{\omega\neq\nu} H(\omega, \nu) &amp;= 1, \\   \log \left| \widetilde{\Omega} \right| &amp;= m\log(2) \end{align*}$$</div><p></p>
<p>Plugging this into the above, we have<br>
</p><div class="mathjax-exps">$$\begin{align*}   \inf_T \sup_p R(T,P) \geq \frac{c_3 h^{2\beta+1}}{2} \left( 1 - \frac{\log(2) + c_3 nh^{2\beta+1} m}{m\log(2)} \right). \end{align*}$$</div><p></p>
<p>I claim the above can't give a lower bound sharper (larger) than <span class="mathjax-exps">$O(\tfrac{1}{n})$</span>. To see why, note the following:</p>
<ol>
<li>
<p>If <span class="mathjax-exps">$nh^{2\beta+1}$</span> is larger, then the LB is negative.</p>
<ul>
<li>So, want <span class="mathjax-exps">$h=O\left( n^{-\frac{1}{2\beta+1}} \right)$</span>.</li>
</ul>
</li>
<li>
<p>In the best case, the RHS is no more than <span class="mathjax-exps">$c_3 h^{2\beta+1}/2$</span>.</p>
</li>
</ol>
<p>Combining these two observations, the lower bound is never going to be sharper than <span class="mathjax-exps">$O(\tfrac{1}{n})$</span>.</p>
</li>
</ul>
<p>From what you saw in STAT 581, you might expect this LB be to loose. In particular, consider the parametric problem where <span class="mathjax-exps">$f_Q: \widetilde{\mathcal{F}} := \left\{ x \mapsto \beta x: \beta\in [0,L] \right\}$</span></p>
<p>![plot of f_Q](/jessica-phillips/img<br>
/IMG_0361.jpg)</p>
<p>From last quarter, we know that the MLE satisfies<br>
</p><div class="mathjax-exps">$$\sqrt{n} (\hat{\beta - \beta}) \stackrel{d}{\to} N(0,\sigma^2)$$</div><p></p>
<p>Hence,<br>
</p><div class="mathjax-exps">$$(\hat{\beta} - \beta)^2 = O_p(\tfrac{1}{n}).$$</div><p></p>
<p>Hence, it's reasonable to expect that<br>
</p><div class="mathjax-exps">$$E_{Q^n} \left[ \int (\hat{\beta}_x - \beta_x)^2 dx \right] = O(\tfrac{1}{n}).$$</div><p></p>
<br>
<ul>
<li><strong>Choice 2.</strong> Idea: find an <span class="mathjax-exps">$\widetilde{\Omega} \subseteq \{ 0,1 \}^m$</span> such that <span class="mathjax-exps">$\log \left| \widetilde{\Omega} \right|$</span> and <span class="mathjax-exps">$\min_{\omega\neq \nu} H(\omega,\nu)$</span> are both large.</li>
</ul>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#aa46e0; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Varshamov-Gilbert Lemma.</strong> If <span class="mathjax-exps">$m\geq 0$</span>, then there exists a subset <span class="mathjax-exps">$\widetilde{\Omega} \subseteq \{ 0,1 \}^m$</span> such that <span class="mathjax-exps">$\left| \widetilde{\Omega} \right| \geq 2^{m/8}$</span> and <span class="mathjax-exps">$\min_{\omega\neq\nu} H(\omega, \nu) \geq \tfrac{m}{8}$</span>.</p>
<p>![image for vs-lemma](/jessica-phillips/img<br>
/IMG_0364.jpg)</p>
</div>
<br>
<p>Choosing <span class="mathjax-exps">$\widetilde{\Omega}$</span> as in the lemma, our <span class="mathjax-exps">$\boxed{\textcolor{red}{\text{lower bound}}}$</span><br>
</p><div class="mathjax-exps">$$\begin{align*}   \inf_T \sup_p R(T,P) &amp;\geq \frac{c_3 h^{2\beta+1} m}{16} \left( 1 - \frac{\log(2) + 8c_3 nh^{2\beta+1}}{m\log(2) / 8} \right) \\       &amp;= \frac{c_3 h^{2\beta+1} m}{16} \left( 1 - \frac{8}{m} - \frac{8c_3 nh^{2\beta+1}}{\log(2)} \right). \end{align*}$$</div><p></p>
<p>We'll choose <span class="mathjax-exps">$m$</span> as large as possible (i.e., <span class="mathjax-exps">$\lfloor{m L \frac{1}{h} - 1\rfloor}$</span>). Approximating this as <span class="mathjax-exps">$m\approx \frac{1}{h}$</span>, we find<br>
</p><div class="mathjax-exps">$$LB \approx \frac{c_3 h^{2\beta}}{16} \left( 1 - 8h - \frac{8 c_3 nh^{2\beta + 1}}{\log(2)} \right) \tag{$\bigstar$}.$$</div><p></p>
<p>For <span class="mathjax-exps">$(\bigstar)$</span> to be nonnegative, we need <span class="mathjax-exps">$h\leq c_4 n^{-\frac{1}{2\beta+1}}$</span> for an appropriately specified <span class="mathjax-exps">$c_4$</span>. Letting <span class="mathjax-exps">$h = \frac{c_4}{2} n^{- \frac{1}{2\beta+1}}$</span>, we find that there's a <span class="mathjax-exps">$c_5&gt;0$</span> such that<br>
</p><div class="mathjax-exps">$$\inf_T \sup_p R(T,P) \geq c_5 n^{-\frac{2\beta}{2\beta+1}}.$$</div><p></p>
<p>For example, if <span class="mathjax-exps">$\beta=2$</span>, then we've derived an <span class="mathjax-exps">$n^{-4/5}$</span>-rate lower bound on the minimax risk. [Note: This is tight (in terms of rates)!]</p>
<p>This means <span class="mathjax-exps">$n^{-4/5}$</span> is the sharpest we can get and there is no lower bound that is asymptotically larger.</p>
<hr>
<p>Listen to Alex's comments on why minimax lower bounds are useful at 18:00 in the Zoom lecture.</p>
<p></p>
<br>
<p style="border-spacing:0px; page-break-before: always">
</p><h2 id="uchapter-2u-kernel-density-estimation"><u>Chapter 2.</u> Kernel Density Estimation. </h2>
<p><big><strong>Chapter 2 Table of Contents</strong></big></p>
<ul>
<li><a href="chapter2\chapter2.md#general-form-of-the-kernel-density-estimator-kde">General form of the Kernel Density Estimator (KDE)</a>
<ul>
<li><a href="chapter2\chapter2.md#studying-%5Chatfhx0-as-an-estimator-of-fx0">Studying <span class="mathjax-exps">$\hat{f}_h(x_0)$</span> as an estimator of <span class="mathjax-exps">$f(x_0)$</span></a></li>
</ul>
</li>
<li><a href="chapter2\chapter2.md#%5Cred%5Ctextbfbias-of-the-kde"><span class="mathjax-exps">$\textcolor{red}{\textbf{Bias}}$</span> of the KDE</a></li>
<li><a href="chapter2\chapter2.md#%5Cblue%5Ctextvariance-of-the-kde"><span class="mathjax-exps">$\textcolor{blue}{\text{Variance}}$</span> of the KDE:</a></li>
<li><a href="chapter2\chapter2.md#generalizations-and-conclusions">Generalizations and Conclusions</a>
<ul>
<li><a href="#generalizations">Generalizations</a></li>
</ul>
</li>
</ul>
<p><strong>Other Sections:</strong> <a href="#uchapter-1u-minimaxity">Chapter 1</a>, <a href="#uchapter-3u-concentration-inequality">Chapter 3</a>, <a href="#uchapter-4-part-1u">Chapter 4 - Part 1</a>, <a href="#uchapter-4-part-2u">Chapter 4 - Part 2</a></p>
<br>
<p></p><p><strong>References:</strong></p>
<ul>
<li>Chapter 24 of var der Vaart</li>
<li>Section 6.3 of Wasserman's <em>All of Statistics</em></li>
</ul>
<br>
<p><strong>Setting:</strong></p>
<ul>
<li>Observe <span class="mathjax-exps">$X_1,X_2,\dots,X_n \stackrel{iid}{\sim} Q$</span>
<ul>
<li>Let <span class="mathjax-exps">$F(x) := Q\{ X\leq x \}$</span> be the cumulative density function</li>
<li>Let <span class="mathjax-exps">$f(x)$</span> denote the probability density function
<ul>
<li>Note that we are assuming that we are always working with a continuous distribution</li>
</ul>
</li>
</ul>
</li>
</ul>
<br>
<p><strong>Our goal:</strong> Estimate <span class="mathjax-exps">$f$</span> at a point <span class="mathjax-exps">$x_0$</span>.</p>
<ul>
<li>
<p>Given that <span class="mathjax-exps">$f(x_0) = \left. \frac{d}{dx} F(x) \right|_{x=x_0}$</span>, we might consider using a plugin estimator<br>
</p><div class="mathjax-exps">$$\hat{f}(x_0) = \frac{d}{dx} \hat{F}(x_0). \tag{1}$$</div><p></p>
<p>One way to estimate <span class="mathjax-exps">$F$</span> is via the empirical CDF<br>
</p><div class="mathjax-exps">$$\hat{F}(x_0) := \frac{1}{n} \sum_{i=1}^{n} I \left\{ X_i \leq x_0 \right\}.$$</div><p></p>
</li>
<li>
<p>We know that <span class="mathjax-exps">$\hat{F}$</span> is a "good" estimator of <span class="mathjax-exps">$F$</span>.</p>
<ul>
<li>e.g., <span class="mathjax-exps">$\sqrt{n} \left[ \hat{F}(x_0) - F(x_0) \right] \stackrel{d}{\to} N(0,\sigma^2)$</span>.<br>
<br></li>
</ul>
</li>
<li>
<p><u>But:</u> estimating <span class="mathjax-exps">$f(x_0)$</span> via <span class="mathjax-exps">$(1)$</span> turns out to be a very bad idea. To see why, refer to the image below.<br>
<br></p>
<p><img src="chapter2\figures\IMG_0366.jpg" alt="why estimating f(x_0) via (1) is bad"></p>
<br>
</li>
<li>
<p>Another possible estimator uses that<br>
</p><div class="mathjax-exps">$$f(x_0) = \lim_{h\to 0} \frac{F(x_0 + h) - F(x_0 - h)}{2h},$$</div><p></p>
<p>and so, for <span class="mathjax-exps">$h&gt;0$</span> small,<br>
</p><div class="mathjax-exps">$$f(x_0) \approx \frac{F(x_0+h) - F(x_0-h)}{2h}.$$</div><p></p>
<p>This suggest the estimator<br>
</p><div class="mathjax-exps">$$\begin{align*}   \hat{f}(x_0) &amp;= \frac{\hat{F}(x_0+h) - \hat{F}(x_0-h)}{2h} \\       &amp;= \frac{1}{2nh} \sum_{i=1}^{n} I \{ x_0 - h &lt; X_i \leq x_0 + h \} \\       &amp;\stackrel{(a.s.)}{=} \frac{1}{nh} \sum_{i=1}^{n} \frac{1}{2} \, I \left\{ \frac{|X_i - x_0 | }{h} \leq 1 \right\}. \end{align*}$$</div><p></p>
<p>Below is an example of what a simple kernel estimator might look like in this case.</p>
<br>
<p><img src="chapter2\figures\IMG_0368.jpg" alt="plot of empirical pdf vs. x"></p>
<br>
<p><strong>Note:</strong> This estimate of <span class="mathjax-exps">$f$</span> is not smooth because it is not continuous <em>or</em> differentiable.</p>
</li>
</ul>
<p><strong>Question</strong>: Can we define a smoother estimate of <span class="mathjax-exps">$f$</span>?</p>
<p><strong>Answer:</strong> Yes!</p>
<ul>
<li>Let <span class="mathjax-exps">$K: \mathbb{R} \to \mathbb{R}$</span> be a kernel, that is, a function satisfying <span class="mathjax-exps">$\int K(u) du = 1$</span>.</li>
</ul>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#42801f; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Definition.</strong> An <span class="mathjax-exps">$s^{th}$</span> order kernel <span class="mathjax-exps">$K$</span> satisfies</p>
<ul>
<li><span class="mathjax-exps">$\int u^r K(u) du = 0$</span> for <span class="mathjax-exps">$r=1,2,\dots,s-1$</span>;</li>
<li><span class="mathjax-exps">$|\int u^s K(u) du &lt; \infty$</span>.
<ul>
<li>This is just saying that <span class="mathjax-exps">$\int K(u) u du = 0$</span></li>
</ul>
</li>
</ul>
</div>
<br>
<p><strong>Note:</strong> We call <span class="mathjax-exps">$h$</span> the bandwidth of the kernel density estimator.</p>
<ul>
<li>
<p>If <span class="mathjax-exps">$K$</span> is symmetric about zero [<span class="mathjax-exps">$K(u) = K(-u)$</span>] then <span class="mathjax-exps">$K$</span> is necessarily at least a second-order kernel.</p>
</li>
<li>
<p>Using higher-order kernels (<span class="mathjax-exps">$s&gt;2$</span>) can lead to estimators with lower bias.</p>
</li>
</ul>
<br>
<h3 id="general-form-of-the-kernel-density-estimator-kde">General form of the Kernel Density Estimator (KDE) </h3>
<p>For a bandwidth <span class="mathjax-exps">$h&gt;0$</span>:<br>
</p><div class="mathjax-exps">$$\begin{align*}      \hat{f}_h(x_0) &amp;= \frac{1}{n} \sum_{i=1}^{n} \frac{1}{h} K \left( \frac{X_i - x_0}{h} \right) \\         &amp;= \frac{1}{n} \sum_{i=1}^{n} K_h(X_i-x_0) \end{align*}$$</div><p></p>
<p>where <span class="mathjax-exps">$K_h(u) = \frac{1}{h} K \left( \frac{u}{h} \right)$</span>.</p>
<p>Examples of kernels:</p>
<ol>
<li>
<p><u>Uniform kernel</u>:<br>
</p><div class="mathjax-exps">$$K(u) = \frac{1}{2} I \{ |u|\leq 1 \}. \tag{$\textcolor{blue}{\bigstar}$}$$</div><p></p>
</li>
<li>
<p><u>Epanechnikov kernel</u>:<br>
</p><div class="mathjax-exps">$$K(u) = \frac{3}{4} (1 - u^2) I \{ |u| \leq 1 \}$$</div><p></p>
</li>
<li>
<p><u>Gaussian kernel</u>:<br>
</p><div class="mathjax-exps">$$K(u) = \frac{1}{\sqrt{2\pi}} \exp \left[ -\frac{u^2}{2} \right]$$</div><p></p>
</li>
</ol>
<p><strong>Note:</strong> All of these kernels are second-order.</p>
<br>
<h4 id="studying-hatf_hx_0-as-an-estimator-of-fx_0">Studying <span class="mathjax-exps">$\hat{f}_h(x_0)$</span> as an estimator of <span class="mathjax-exps">$f(x_0)$</span> </h4>
<p>We'll quantify performance in terms of MSE:<br>
</p><div class="mathjax-exps">$$E \left[ \left\{ \hat{f}_h(x_0) - f(x_0) \right\}^2 \right] = \underbrace{\textcolor{red}{ \left\{ E[\hat{f}_h(x_0)] - f(x_0) \right\}^2}}_{\text{bias}^2} + \underbrace{\textcolor{blue}{var(\hat{f}_h(x_0))}}_{\text{variance}}$$</div><p></p>
<p>We'll suppose that <span class="mathjax-exps">$f$</span> belong to a <span class="mathjax-exps">$(\beta,L)$</span> Hölder class with <span class="mathjax-exps">$\beta=2$</span> and <span class="mathjax-exps">$L&gt;0$</span>.</p>
<p><strong>Recall:</strong> Saying that <span class="mathjax-exps">$f$</span> is <span class="mathjax-exps">$(2,L)$</span> Hölder means that<br>
</p><div class="mathjax-exps">$$\left| f'(x_1) - f'(x_2) \right| \leq L |x_1 - x_2| \hspace{2em} \forall x_1, x_2.$$</div><p></p>
<p><strong>Note:</strong> All of the calculations we do today will still go through if <span class="mathjax-exps">$f$</span> isn't Hölder, but there exists a neighborhood of <span class="mathjax-exps">$x_0$</span> such that the restriction of <span class="mathjax-exps">$f$</span> to that neighborhood is <span class="mathjax-exps">$(2,L)$</span> Hölder.</p>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#000000; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>We'll focus on the case where <span class="mathjax-exps">$K$</span>:</strong></p>
<ul>
<li>is bounded;</li>
<li>is nonnegative;</li>
<li>is second-order; and</li>
<li>has bounded support.</li>
</ul>
<p>We're going to see that choosing a small bandwidth <span class="mathjax-exps">$h&gt;0$</span> yields <span class="mathjax-exps">$\textcolor{red}{\text{low bias}}$</span> and <span class="mathjax-exps">$\textcolor{blue}{\text{high variance}}$</span>, and vice versa.</p>
</div>
<br>
<h3 id="redtextbfbias-of-the-kde"><span class="mathjax-exps">$\textcolor{red}{\textbf{Bias}}$</span> of the KDE </h3>
<p><strong>Recall:</strong> The bias is <span class="mathjax-exps">$\text{Bias} = E[\hat{f}_h(x_0)] - f(x_0)$</span> and <span class="mathjax-exps">$f(x_0) = \int K(u) f(x_0) du$</span> (since <span class="mathjax-exps">$\int K(u) du = 1$</span>). Thus, we have<br>
</p><div class="mathjax-exps">$$\begin{align*}     E\left[ \hat{f}_h(x_0) \right] &amp;= \frac{1}{nh} \sum_{i=1}^{n} E \left[ K \left( \frac{X_i - x_0}{h} \right) \right] \\         &amp;= \frac{1}{h} E \left[ K \left( \frac{X_1 - x_0}{h} \right) \right] \tag{identically distributed} \\         &amp;= \frac{1}{h} \int K \left( \frac{x_1 - x_0}{h} \right) f(x_1) dx_1 \tag{$u = \tfrac{x_1-x_0}{h}$}m \\         &amp;= \int K(u) f(x_0 + uh) du. \end{align*}$$</div><p></p>
<p>Recalling that <span class="mathjax-exps">$\int K(u) du = 1$</span>, we have<br>
</p><div class="mathjax-exps">$$\text{Bias} = \int K(u) \underbrace{[f(x_0+ uh) - f(x_0)]}_{\textcolor{purple}{\bigstar}} du.$$</div><p></p>
<p>By the mean value theorem, there exists <span class="mathjax-exps">$\tilde{x}_{uh}$</span> between <span class="mathjax-exps">$x_0+uh$</span> and <span class="mathjax-exps">$x_0$</span> such that<br>
</p><div class="mathjax-exps">$$\textcolor{purple}{\bigstar} = uh f'(\tilde{x}_{uh}).$$</div><p></p>
<p>Hence,<br>
</p><div class="mathjax-exps">$$\begin{align*}     \text{Bias} &amp;= \int K(u) uh \, f'(\tilde{x}_{uh}) du \\         &amp;= \underbrace{\int K(u) uh f'(x_0) du}_{\substack{\textcolor{purple}{h f'(x_0) \int K(u) u du} \\ \textcolor{purple}{=0 \text{ b/c } K \text{ is } 2^{nd} \text{-order}}}} + \int K(u) uh \left[ f'(\tilde{x}_{uh}) - f'(x_0) \right] du. \end{align*}$$</div><p></p>
<p>We're leveraging the fact that <span class="mathjax-exps">$\tilde{x}_{uh}$</span> is close to <span class="mathjax-exps">$x_0$</span>, as can be seen on the number line below:<br>
<small></small></p><small>
<p><img src="chapter2\figures\IMG_0375.jpg" alt="number line"></p>
</small>
<p>Hence,<br>
</p><div class="mathjax-exps">$$\begin{align*}     |\text{Bias}| &amp;= \left| \int K(u) uh  [f'(\tilde{x}_{uh})  - f'(x_0)] du \right| \\         &amp;\leq h \int K(u) \vert u| |f'(\tilde{x}_{uh}) - f'(x_0)| du \\         &amp;\leq L h \int K(u) |u| \underbrace{|\tilde{x}_{uh} - x_0 |}_{\textcolor{orange}{|u| h}} du \\         &amp;\leq Lh \underbrace{\int K(u) u^2 du}_{:=\sigma_k^2 \text{ (constant)}}. \end{align*}$$</div><p></p>
<p>Hence,<br>
</p><div class="mathjax-exps">$$\textcolor{red}{\boxed{\text{Bias}^2 \leq L \sigma_k^4 h^4}}.$$</div><p></p>
<h3 id="bluetextvariance-of-the-kde"><span class="mathjax-exps">$\textcolor{blue}{\text{Variance}}$</span> of the KDE: </h3>
<p></p><div class="mathjax-exps">$$\begin{align*}     var[\hat{f}_h(x_0)] &amp;= var \left[ \frac{1}{nh} \sum_{i=1}^{n} K \left( \frac{X_i - x_0}{h} \right) \right] \\         &amp;= \frac{1}{n^2h^2} \sum_{i=1}^{n} var\left[ K \left( \frac{X_i - x_0}{h} \right) \right] \tag{ind.} \\         &amp;= \frac{1}{nh^2} var \left[ K \left( \frac{X_1 - x_0}{h} \right) \right] \tag{ident.} \\         &amp;\leq \frac{1}{nh^2} E \left[ K\left( \frac{x_1 - x_0}{h} \right)^2 \right] \\         &amp;= \frac{1}{nh^2} \int K \left( \frac{x_1-x_0}{h} \right)^2 f(x_1) dx_1 \\         &amp;= \frac{1}{nh} \underbrace{\int K(u)^2 f(x_0 + uh) du}_{\textcolor{purple}{(\bigstar\bigstar})} \hspace{2em} u= \frac{x_1 - x_0}{h} \tag{2} \end{align*}$$</div><p></p>
<p>We'll study <span class="mathjax-exps">$\textcolor{purple}{(\bigstar\bigstar)}$</span> in what follows. To do this,  we'll use the following two facts:</p>
<ol>
<li><span class="mathjax-exps">$f$</span> is Hölder continuous, and therefore, continuous in general;</li>
<li><span class="mathjax-exps">$K$</span> has bounded support.</li>
</ol>
<p>Let <span class="mathjax-exps">$k_1:= \inf\left\{ u: K(u)&gt;0 \right\}$</span> and <span class="mathjax-exps">$k_2:= \sup \left\{ u: K(u)&gt;0 \right\}$</span>. We have that<br>
</p><div class="mathjax-exps">$$\begin{align*}     \textcolor{purple}{\bigstar\bigstar} &amp;= \int_{[k_1,k_2]} K(u)^2 f(x_0 + uh) du \\         &amp;\leq \left[ \sup_{u\in[k_1,k_2]} f(x_0+uh) \right] \int_{[k_1,k_2]} K(u)^2 du. \end{align*}$$</div><p></p>
<p>If <span class="mathjax-exps">$h\leq 1$</span>, then this shows<br>
</p><div class="mathjax-exps">$$\begin{align*}     &amp;\leq \underbrace{\left[ \sup_{t\in[k_1,k_2]} f(x_0+t) \right] \int_{[k_1,k_2]} K(u)^2 du}_{\textcolor{orange}{\widetilde{C}}}. \end{align*}$$</div><p></p>
<p>Recall that continuous functions are bounded over compact sets. Also remember that we are trying to find a bound that does not depend on <span class="mathjax-exps">$h$</span>. Also note that since <span class="mathjax-exps">$K$</span> is bounded, we know that the above will be a finite constant.</p>
<p>Plugging the above into <span class="mathjax-exps">$(2)$</span>,<br>
</p><div class="mathjax-exps">$$\textcolor{blue}{var(\hat{f}_{h}(x_0)) \leq \frac{\widetilde{C}}{nh}}.$$</div><p></p>
<h3 id="generalizations-and-conclusions">Generalizations and Conclusions </h3>
<p><strong>Conclusion:</strong> Plugging in our bounds in <span class="mathjax-exps">$\textcolor{red}{\boxed{\text{Bias}^2}}$</span> and <span class="mathjax-exps">$\textcolor{blue}{\boxed{var}}$</span>, we find that<br>
</p><div class="mathjax-exps">$$\textcolor{purple}{\sup_Q} \text{MSE} \leq \textcolor{red}{L^2 \sigma_k^2 h^4} + \textcolor{blue}{\frac{\widetilde{C}}{nh}} \tag{$\textcolor{purple}{\blacktriangle}$}$$</div><p></p>
<p><strong>Note from Alex:</strong> The first terms shrinks as <span class="mathjax-exps">$h$</span> becomes small and the second grows. Setting the two terms equal to each other and solving for <span class="mathjax-exps">$h$</span> will provide a note describing how <span class="mathjax-exps">$h$</span> should change with <span class="mathjax-exps">$n$</span> to make the bound as tight as possible. Setting the two terms to each other and solving for <span class="mathjax-exps">$h$</span> is the best way to balance out the two terms.<br>
</p><div class="mathjax-exps">$$L^2 \sigma_k^4 h^4 = \frac{\widetilde{C}}{nh} \implies h = Cn^{-1/5}$$</div><p></p>
<p>for some <span class="mathjax-exps">$C$</span>. Plugging this into our bound on the MSE, we find that choosing the bandwidth yields<br>
</p><div class="mathjax-exps">$$MSE = O(n^{-4/5}).$$</div><p></p>
<p><strong>Note to self:</strong> We are studying the bias-variance tradeoff, so increasing the variance will increase the bias, and vice-versa. Our goal is to choose an <span class="mathjax-exps">$h$</span> that balances out these two terms so neither of them are too large. Also note that removing the supremum from <span class="mathjax-exps">$(\textcolor{purple}{\blacktriangle})$</span> makes it possible for the upper bound to be loose under certain conditions.</p>
<br>
<h4 id="generalizations">Generalizations </h4>
<ol>
<li>
<p><u>1-dimensional settings with different levels of smoothness:</u><br>
<br></p>
<ul>
<p>If <span class="mathjax-exps">$f$</span> belong to a <span class="mathjax-exps">$(\beta,L)$</span> Hölder class, then similar arguments show that when we choose an appropriate <span class="mathjax-exps">$h$</span><br>
</p><div class="mathjax-exps">$$\text{MSE} = O\left( n^{-\frac{2\beta}{2\beta+1}} \right)$$</div><p></p>
<p>if a kernel of sufficiently high order is user</p>
<p><strong>Note:</strong> Large values of <span class="mathjax-exps">$\beta$</span> <span class="mathjax-exps">$\implies$</span> more smoothness.</p>
</ul>
</li>
<li>
<p><u><span class="mathjax-exps">$d$</span>-dimensional settings:</u><br>
<br></p>
<ul>
<p>Suppose <span class="mathjax-exps">$X$</span> is <span class="mathjax-exps">$d$</span>-dimensional. A KDE in this case takes the form<br>
</p><div class="mathjax-exps">$$\hat{f}_h(x_0) = \frac{1}{nh^d} \sum_{i=1}^{n} \prod_{j=1}^{d} K \left( \frac{X_{i,j} - x_{0,j}}{h} \right).$$</div><p></p>
<p>If <span class="mathjax-exps">$f$</span> is <span class="mathjax-exps">$\textcolor{blue}{\beta\text{-times differentiable}}$</span> and all partial derivatives up to order <span class="mathjax-exps">$\beta$</span> are bounded, then for proper choice of bandwidth <span class="mathjax-exps">$h$</span>, we have<br>
</p><div class="mathjax-exps">$$\text{MSE} = O\left( n^{-\frac{2\textcolor{blue}{\beta}}{2\textcolor{blue}{\beta}+\textcolor{red}{d}}} \right)$$</div><p></p>
<p>To see this more clearly, reference the below table.</p>
<table>
<thead>
<tr>
<th style="text-align:center">dimension (<span class="mathjax-exps">$d$</span>)</th>
<th style="text-align:center">MSE upper bound</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center"><span class="mathjax-exps">$n^{-0.80}$</span></td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center"><span class="mathjax-exps">$n^{-0.67}$</span></td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center"><span class="mathjax-exps">$n^{-0.50}$</span></td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center"><span class="mathjax-exps">$n^{-0.29}$</span></td>
</tr>
</tbody>
</table>
<p><strong>One way of thinking about the exponent:</strong> If <span class="mathjax-exps">$\text{MSE} = n^{-\alpha}$</span>, then, when <span class="mathjax-exps">$n$</span> is large, to halve your MSE, you need to collect to <span class="mathjax-exps">$2^{1/\alpha}$</span> as many observations.</p>
</ul>
</li>
</ol>
<p></p>
<br>
<p style="border-spacing:0px; page-break-before: always">
</p><h2 id="uchapter-3u-concentration-inequality"><u>Chapter 3.</u> Concentration Inequality. </h2>
<p><big><strong>Chapter 3 Table of Contents</strong></big></p>
<ul>
<li><a href="#bounds-on-moments">Bounds on Moments</a></li>
<li><a href="#sub-gaussian-random-variables">Sub-Gaussian Random Variables</a>
<ul>
<li><a href="#alternative-characterizations-of-sub-gaussianity">Alternative Characterizations of Sub-Gaussianity</a></li>
</ul>
</li>
<li><a href="#sub-exponential-random-variables">Sub-Exponential Random Variables</a>
<ul>
<li><a href="#an-alternative-characterization-of-sub-exponential-random-variables">An alternative characterization of sub-exponential random variables</a></li>
</ul>
</li>
<li><a href="#bounded-differences-inequality">Bounded Differences Inequality</a></li>
</ul>
<p><strong>Other Sections:</strong> <a href="#uchapter-1u-minimaxity">Chapter 1</a>, <a href="#uchapter-2u-kernel-density-estimation">Chapter 2</a>, <a href="#uchapter-4-part-1u">Chapter 4 - Part 1</a>, <a href="#uchapter-4-part-2u">Chapter 4 - Part 2</a></p>
<br>
<p></p><p><strong>References:</strong>  Chapter 2 of the Wainwright textbook</p>
<p><strong>Note:</strong> In this chapter, it is often not required that the data be identically distributed, and independence is typically all we need.</p>
<p><strong>Objection/Motivation:</strong> Suppose <span class="mathjax-exps">$X_1,X_2,\dots,X_n$</span> are independent random variables.</p>
<ul>
<li>
<p>In many cases, it will be useful to find bounds on some function <span class="mathjax-exps">$f$</span> of <span class="mathjax-exps">$X_1,\dots,X_n$</span>.</p>
<ul>
<li>That is, we want to bound<br>
<div class="mathjax-exps">$$P \{ f(X_1,\dots,X_n) \geq t \} \hspace{2em} \text{for some } t&gt;0. \tag{1}$$</div></li>
</ul>
</li>
<li>
<p>For the first part of this chapter, we'll focus on <span class="mathjax-exps">$f$</span> of the form<br>
</p><div class="mathjax-exps">$$f: \underbrace{(x_1,\dots,x_n)}_{\textcolor{blue}{x_1^n}} \mapsto \frac{1}{n} \sum_{i=1}^{n} x_i$$</div><p></p>
<p>or<br>
</p><div class="mathjax-exps">$$f: x_1^n \mapsto \left| \frac{1}{n} \sum_{i=1}^{n} x_i - E \left[ \frac{1}{n} \sum_{i=1}^{n} X_i \right] \right|.$$</div><p></p>
</li>
<li>
<p>We could consider upper bounding <span class="mathjax-exps">$(1)$</span> using asymptotics.<br>
<br></p>
<p>For example, if <span class="mathjax-exps">$X_1,\dots,X_n$</span> are not just independent, but also identically distributed, we have <span class="mathjax-exps">$\mu := E[X]$</span> and <span class="mathjax-exps">$\sigma^2 = var(X)$</span>, then the CLT says that<br>
</p><div class="mathjax-exps">$$P \left\{ \bar{X}_n \geq \mu + \frac{t\sigma}{\sqrt{n}} \right\} \stackrel{n\to\infty}{\longrightarrow} 1 - \Phi(t),$$</div><p></p>
<p>where <span class="mathjax-exps">$\Phi$</span> represents the CDF of the standard normal distribution.<br>
<br></p>
<p><u>But:</u> we usually only have a finite sample size, and so these guarantees are only approximate.</p>
</li>
</ul>
<p><u><strong>Question:</strong></u> What can we say in finite samples? For example, can we say that<br>
</p><div class="mathjax-exps">$$P \left\{ \bar{X}_n \geq \mu + t \right\} \leq \circ$$</div><p></p>
<p>During this chapter, we'll obtain bounds on <span class="mathjax-exps">$(1)$</span> using:</p>
<ol>
<li>
<p><u>Bounds on moments:</u> Markov, Chebyshev.</p>
</li>
<li>
<p><u>Bounds on the MGF:</u></p>
<ul>
<li>Chernoff bound,</li>
<li>Hoeffding,</li>
<li>sub-Gaussian random variables,</li>
<li>sub-exponential random variables, and</li>
<li>Bernstein's inequality.</li>
</ul>
</li>
<li>
<p><u>Martingale arguments:</u> Azuma-Hoeffding bound, bounded differences inequality.</p>
</li>
</ol>
<br>
<h3 id="bounds-on-moments">Bounds on Moments </h3>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#43b7d1; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Theorem (Markov).</strong> If (1) <span class="mathjax-exps">$X\geq 0$</span>, (2) <span class="mathjax-exps">$E[X]&lt;\infty$</span>, and (3) <span class="mathjax-exps">$t&gt;0$</span>, then<br>
</p><div class="mathjax-exps">$$P \left\{ X\geq t \right\} \leq \frac{E[X]}{t}.$$</div><p></p>
<br>
<p><strong>Proof.</strong> For any <span class="mathjax-exps">$t&gt;0$</span>,<br>
</p><div class="mathjax-exps">$$\begin{align*}     P \left\{ X\geq t \right\} &amp;= \int_t^\infty \textcolor{purple}{1} dP(x) \\         &amp;\leq \int_t^\infty \textcolor{purple}{\frac{x}{t}} dP(x) \hspace{2em} \textcolor{purple}{ \left( 1 \leq \frac{x}{t} \text{ whenever } t\leq x \right) } \\         &amp;\leq \int_0^\infty \frac{x}{t} dP(x) \\         &amp;= \frac{E[X]}{t}. \end{align*}$$</div><p></p>
</div>
<br>
<p><strong>Note:</strong> <u>Note</u> all of our concentration inequalities use Markov's inequality as a building block.</p>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#43b7d1; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Theorem.</strong> Suppose that <span class="mathjax-exps">$E|X|&lt;\infty$</span>, where <span class="mathjax-exps">$h: [0,\infty) \to [0,\infty)$</span>, is a nondecreasing function for which <span class="mathjax-exps">$h(t)&gt;0$</span> for all <span class="mathjax-exps">$t&gt;0$</span>, and <span class="mathjax-exps">$E[h(|X- E[X]|)] &lt; \infty$</span>. For all <span class="mathjax-exps">$t&gt;0$</span>,<br>
</p><div class="mathjax-exps">$$P \left\{ |X- E[X]| \geq t \right\} \leq \frac{E[h(|X-E[X]|)]}{h(t)}.$$</div><p></p>
<br>
<p><strong>Proof.</strong> Because <span class="mathjax-exps">$h$</span> is nondecreasing,<br>
</p><div class="mathjax-exps">$$\{ |X-E[X]| \geq t \} \subseteq \left\{ h(|X-E[X]|) \geq h(t) \right\}.$$</div><p></p>
<p>Hence,<br>
</p><div class="mathjax-exps">$$\begin{align*}     P \left\{ |X- E[X]| \geq t \right\} &amp;\leq P \left\{ h(|X-E[X]|) \geq h(t) \right\} \\         &amp;\leq \frac{E[h(|X-E[X]|)]}{h(t)}. \hspace{4em} \textcolor{purple}{\text{(by Markov)}} \end{align*}$$</div><p></p>
</div>
<br>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#0000ff; padding:0.75em; padding-bottom: 0em;">
<p><big><strong>Example:</strong> Chebyshev.</big><br>
Let <span class="mathjax-exps">$h(t) = t^2$</span>. Then,<br>
</p><div class="mathjax-exps">$$P \left\{ |X-E[X]| \geq t \right\} \leq \frac{E[|X-E[X]|^2]}{t^2} = \frac{var(X)}{t^2}.$$</div><p></p>
<p>In fact, we can similarly take <span class="mathjax-exps">$h(t) = t^k$</span> for any <span class="mathjax-exps">$k&gt;0$</span> to find<br>
</p><div class="mathjax-exps">$$P \left\{ |X-E[X]| \geq t \right\} \leq \frac{E[|X-E[X]|^k]}{t^k}.$$</div><p></p>
</div>
<br>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#000000; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Remark.</strong> Suppose <span class="mathjax-exps">$\ell&lt;k$</span> and both <span class="mathjax-exps">$E[|X-E[X]|^\ell]$</span> and <span class="mathjax-exps">$E[|X-E[X]|^k]$</span> are both finite. For large <span class="mathjax-exps">$t$</span>, the bound based on <span class="mathjax-exps">$k$</span> will be much sharper.</p>
<ul>
<li>Why? <span class="mathjax-exps">$\frac{1}{t^k}$</span> decays much faster than <span class="mathjax-exps">$\frac{1}{t^\ell}$</span> as <span class="mathjax-exps">$t$</span> grows.</li>
</ul>
<p>This is somewhat intuitive: having <span class="mathjax-exps">$E[|X-E[X]|^k] &lt;\infty$</span> is a stronger condition that having <span class="mathjax-exps">$E[|X-E[X]|^\ell]$</span>.</p>
</div>
<br>
<p><u><strong>Question:</strong></u> Can we get even sharper tail inequalities than the one above if we assume even more about <span class="mathjax-exps">$X$</span>?</p>
<p><u><strong>Answer:</strong></u> Yes! Provided <span class="mathjax-exps">$X$</span> has a moment-generating function.</p>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#43b7d1; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Theorem (Chernoff).</strong> Suppose that <span class="mathjax-exps">$X$</span> has a MGF in a neighborhood of zero, meaning that there exists <span class="mathjax-exps">$b&gt;0$</span> such that<br>
</p><div class="mathjax-exps">$$M_X(\lambda) := E \left[ \exp \{ \lambda X \} \right] &lt; \infty$$</div><p></p>
<p>for all <span class="mathjax-exps">$|\lambda| \leq b$</span>. Then, for <span class="mathjax-exps">$t&gt;0$</span> and <span class="mathjax-exps">$\lambda \in (0,b]$</span>,<br>
</p><div class="mathjax-exps">$$\begin{align*}     P \left\{ X - E[X] \geq t \right\} &amp;= P \left\{ e^{\lambda (X-E[X])} \geq e^{\lambda t} \right\} \\     &amp;\leq \frac{E[e^{\lambda(X-E[X])}]}{e^{\lambda t}} \\     &amp;=: \frac{M_{X-\mu}(\lambda)}{e^{\lambda t}} \end{align*}$$</div><p></p>
</div>
<br>
<p>where <span class="mathjax-exps">$\mu = E[X]$</span>. Hence,<br>
</p><div class="mathjax-exps">$$P \{ X-E[X] \geq t \} \leq \inf_{\lambda&gt;0} \frac{M_{X-\mu}(\lambda)}{e^{\lambda t}}.$$</div><p></p>
<p>Equivalently,<br>
</p><div class="mathjax-exps">$$\textcolor{orange}{\boxed{\log P \{ X- E[X] \geq t \} \leq -\sup_{\lambda&gt;0} \left\{ \lambda t - \underbrace{\log M_{x-\mu}(\lambda)}_{\substack{\textcolor{blue}{\text{``cumulant generating}} \\ \textcolor{blue}{\text{function"}}}} \right\}}}$$</div><p></p>
<p>Remark: Apply Chernoff twice, one to the random variable <span class="mathjax-exps">$X$</span> and once to <span class="mathjax-exps">$-X$</span>, we see that<br>
</p><div class="mathjax-exps">$$\begin{align*}     P \left\{ |X-E[X]| \geq t \right\} \leq P \left\{ X - E[X] \geq t \right\} + P \left\{ X-E[X] \geq -t \right\}. \end{align*}$$</div><p></p>
<p>We can use Chernoff to bound each of the two tail probabilities on the right.</p>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#0000ff; padding:0.75em; padding-bottom: 0em;">
<p><big><strong>Example:</strong> Gaussian.</big></p>
<p>Suppose <span class="mathjax-exps">$X\sim N(\mu,\sigma^2)$</span>. In this case,<br>
</p><div class="mathjax-exps">$$M_{X-\mu}(\lambda) = \exp \left\{ \frac{\lambda^2 \sigma^2}{2} \right\},$$</div><p></p>
<p>as shown on slide 14 of the lecture notes.</p>
<p>Plugging the form of <span class="mathjax-exps">$M_{X-\mu}(\lambda)$</span> info <span class="mathjax-exps">$\textcolor{orange}{\boxed{\text{Chernoff}}}$</span>, we find that<br>
</p><div class="mathjax-exps">$$\begin{align*}     \log P \left\{ X- \mu \geq t \right\} &amp;\leq - \sup_{\lambda&gt;0} \left\{ \lambda t - \log M_{X-\mu} (\lambda) \right\} \\         &amp;= - \sup_{\lambda&gt;0} \left\{ \lambda t - \frac{\lambda^2\sigma^2}{2} \right\}. \end{align*}$$</div><p></p>
<p>Solving for the <span class="mathjax-exps">$\lambda^*$</span> that maximizes the above, we find that <span class="mathjax-exps">$\lambda^* = \frac{t}{\sigma^2}$</span>. Hence,<br>
</p><div class="mathjax-exps">$$\begin{align*}     \log P &amp;\left\{ X- \mu \geq t \right\} \leq \frac{t}{-2\sigma^2} \\         &amp;\implies \log P \left\{ X- \mu \geq t \right\} \leq \exp \left\{ - \frac{t^2}{2\sigma^2} \right\}. \end{align*}$$</div><p></p>
<p><img src="chapter3\figures\IMG_0377.jpg" alt="plot of the upper bound as a function of t"></p>
<p>Note that <span class="mathjax-exps">$UB(t)$</span> represents the upper bound as a function of <span class="mathjax-exps">$t$</span>.</p>
</div>
<br>
<h3 id="sub-gaussian-random-variables">Sub-Gaussian Random Variables </h3>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#42801f; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Definition.</strong> A random variable <span class="mathjax-exps">$X$</span> is called <span class="mathjax-exps">$\textcolor{blue}{\text{sub-Gaussian (sub-G)}}$</span> with parameter <span class="mathjax-exps">$\sigma^2$</span> if<br>
</p><div class="mathjax-exps">$$\log M_{X-\mu}(\lambda) \leq \frac{\lambda^2\sigma^2}{2} \hspace{2em} \text{for all }  \lambda \in\mathbb{R}.$$</div><p></p>
</div>
<br>
<p>The right side of the inequality in the definition is the cumulant generating function of the distribution.</p>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#000000; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Note:</strong> A <span class="mathjax-exps">$N(\mu,\sigma^2)$</span> random variable is sub-Gaussian with parameter <span class="mathjax-exps">$\sigma^2$</span>.</p>
</div>
<br>
<p>By Chernoff, a sub-Gaussian random variable <span class="mathjax-exps">$X$</span> with parameter <span class="mathjax-exps">$\sigma^2$</span> satisfies<br>
</p><div class="mathjax-exps">$$\textcolor{blue}{\boxed{P\left\{ X - E[X] \geq t \right\} \leq \exp \left\{ - \frac{t^2}{2\sigma^2} \right\}.}}$$</div><p></p>
<p>This is the same bound as we derived for a <span class="mathjax-exps">$N(\mu,\sigma^2)$</span> random variable!<br>
<br></p>
<h4 id="alternative-characterizations-of-sub-gaussianity">Alternative Characterizations of Sub-Gaussianity </h4>
<p><strong>Note:</strong> this can be found in <u>Wainwright, Chapter 2</u>.</p>
<p>A random variable <span class="mathjax-exps">$X$</span> is sub-Gaussian if and only if there exist <span class="mathjax-exps">$\textcolor{blue}{c&gt;0}$</span> and <span class="mathjax-exps">$\textcolor{orange}{s&gt;0}$</span> such that, for all <span class="mathjax-exps">$t&gt;0$</span>,<br>
</p><div class="mathjax-exps">$$P \left\{ |X-E[X]| \geq t \right\} \leq \textcolor{blue}{c} P \left\{ |\textcolor{orange}{s} Z| \geq t \right\},$$</div><p></p>
<p>where <span class="mathjax-exps">$Z\sim N(0,1)$</span>.</p>
<p><strong>Remark.</strong> <span class="mathjax-exps">$\textcolor{orange}{s}$</span> is not necessarily equal to <span class="mathjax-exps">$\sigma$</span>.</p>
<p>Observe the below plot:</p>
<p><img src="chapter3\figures\IMG_0379.jpg" alt="plot"></p>
<p>The probability is the area of the curve in the shaded tails.</p>
<p>The dotted line represents a Gaussian with mean zero where, if we shaded under it, the shaded area under the Gaussian density will always be larger.</p>
<p><strong>Counterexample:</strong> An <span class="mathjax-exps">$\text{Exp}(1)$</span> random variable is not sub-Gaussian. Why?<br>
</p><div class="mathjax-exps">$$P \left\{ X \geq t \right\} = \exp(-t)$$</div><p></p>
<p>The decay behavior is too solve, so we will never find <span class="mathjax-exps">$\textcolor{blue}{c}$</span> and <span class="mathjax-exps">$\textcolor{orange}{s}$</span> such that the above is true. Sub-Gaussianity regards tail decay.</p>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#43b7d1; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Theorem (Hoeffding).</strong> If <span class="mathjax-exps">$X$</span> is a random variable with support <span class="mathjax-exps">$[a,b]$</span> (<span class="mathjax-exps">$|a|, |b| &lt;\infty$</span>), then <span class="mathjax-exps">$X$</span> is sub-Gaussian with parameter <span class="mathjax-exps">$\sigma^2 = \frac{(b-a)^2}{4}$</span>.</p>
<br>
<p><strong>Proof.</strong> Without loss of generality, suppose that <span class="mathjax-exps">$E[X] = 0$</span>. Let <span class="mathjax-exps">$f(\lambda) = \log M_{X-\mu}(\lambda)$</span>. Notings that<br>
</p><div class="mathjax-exps">$$f'(0) = \frac{E[X]}{E \left\{ \exp [(X-\mu)\cdot 0] \right\}} = E[X] = 0,$$</div><p></p>
<p>we have that<br>
</p><div class="mathjax-exps">$$f(\lambda) = \underbrace{\int_0^\infty f'(r) dr}_{\textcolor{purple}{= f(\lambda) - f(0) = f(\lambda)}} = \int_{0}^{\lambda} \underbrace{\int_0^{r} f''(s) ds}_{\textcolor{purple}{f'(r) - f'(0) = f'(r)}} \, dr. \tag*{\textcolor{blue}{(1)}}$$</div><p></p>
<p>Hence, to bound <span class="mathjax-exps">$f$</span>, it's enough to bound <span class="mathjax-exps">$f''$</span>. Note that<br>
</p><div class="mathjax-exps">$$\begin{align*}     f'(\lambda) &amp;= \frac{E[Xe^{\lambda X}]}{E[e^{\lambda X}]}, \\     f''(\lambda) &amp;= \frac{E[X^2 e^{\lambda X}]}{E[e^{\lambda X}]} - \left( \frac{E[Xe^{\lambda X}]}{E[e^{\lambda X}]} \right)^2. \end{align*}$$</div><p></p>
<p>Note that <span class="mathjax-exps">$f''(\lambda)$</span> is the variance of a RV <span class="mathjax-exps">$Z\lambda$</span> with density<br>
</p><div class="mathjax-exps">$$x \mapsto \frac{e^{\lambda x}}{E[e^{\lambda x}]} p(x).$$</div><p></p>
<p>Hence,<br>
</p><div class="mathjax-exps">$$\begin{align*}     f''(\lambda) &amp;= var(Z_\lambda) \\         &amp;= var \left( Z_\lambda - \frac{a+b}{2} \right) \\         &amp;\leq E \left[ \left\{ Z_\lambda - \frac{a+b}{2} \right\}^2 \right] \\         &amp;\leq \frac{(b-a)^2}{4}. \end{align*}$$</div><p></p>
<p>Plugging the above into <span class="mathjax-exps">$\textcolor{blue}{(1)}$</span>,<br>
</p><div class="mathjax-exps">$$f(\lambda) = \int_{0}^{\lambda} \int_{0}^{r} \frac{(b-a)^2}{4} ds \, dr = \frac{(b-a)^2}{4} \cdot \frac{\lambda^2}{2}.$$</div><p></p>
<p>Hence, <span class="mathjax-exps">$X$</span> is sub-Gaussian with parameter <span class="mathjax-exps">$\sigma^2 = \frac{(b-a)^2}{4}$</span>.</p>
</div>
<br>
<p><big><strong>Implication of the Theorem</strong></big><br>
By Chernoff, for any <span class="mathjax-exps">$X$</span> whose support is contained in <span class="mathjax-exps">$[a,b]$</span>, we have<br>
</p><div class="mathjax-exps">$$P \left\{ X- \mu \geq t \right\} \leq \exp \left\{ - \frac{2t^2}{(b-a)^2} \right\}.$$</div><p></p>
<p>This inequality is known as <span class="mathjax-exps">$\textcolor{blue}{\text{Hoeffding's inequality}}$</span>.<br>
<br></p>
<p><big><strong>More general form of Hoeffding's inequality</strong></big><br>
Suppose that <span class="mathjax-exps">$X_1,X_2,\dots, X_n$</span> are independent RVs with support contained in <span class="mathjax-exps">$[a,b]$</span>. In this case, we have<br>
</p><div class="mathjax-exps">$$P \left\{ \bar{X}_n - E[\bar{X}_n] \geq t \right\} \leq \exp \left\{ - \frac{2nt^2}{(b-a)^2} \right\}, \tag*{\textcolor{blue}{(2)}}$$</div><p></p>
<p>where <span class="mathjax-exps">$\bar{X}_n := \frac{1}{n} \sum_i X_i$</span> represents the sample mean.</p>
<p>We'll sketch the proof of the above in the special case where <span class="mathjax-exps">$n=2$</span>. To do this, we'll show that if <span class="mathjax-exps">$X_1$</span> and <span class="mathjax-exps">$X_2$</span> are independent and sub-Gaussian with parameters <span class="mathjax-exps">$\sigma_1^2$</span> and <span class="mathjax-exps">$\sigma_2^2$</span>, then <span class="mathjax-exps">$X_1+X_2$</span> is sub-Gaussian with parameter <span class="mathjax-exps">$\sigma_1^2 + \sigma_2^2$</span>.</p>
<p>To see this,<br>
</p><div class="mathjax-exps">$$\begin{align*}     M_{X_1+X_2} (\lambda) &amp;= E[\exp \{ \lambda(X_1 + X_2) \}] \\         &amp;= E[\exp\{ \lambda X_1 \} \exp \{ \lambda X_2 \}] \\         &amp;= E[\exp\{ \lambda X_1 \}] E[\exp\{ \lambda X_2 \}] \tag*{\textcolor{purple}{(\text{independence})}} \\         &amp;= M_{X_1} (\lambda) M_{X_2}(\lambda). \end{align*}$$</div><p></p>
<p>Similarly,<br>
</p><div class="mathjax-exps">$$M_{X_1 + X_2 - E[X_1 + X_2]}(\lambda) = M_{X_1 - E[X_1]}(\lambda) \cdot M_{X_2 - E[X_2]}(\lambda).$$</div><p></p>
<p>By the sub-Gaussianity of <span class="mathjax-exps">$X_1$</span> and <span class="mathjax-exps">$X_2$</span>, this implies<br>
</p><div class="mathjax-exps">$$\begin{align*}     \log M_{X_1 + X_2 - E[X_1 + X_2]}(\lambda) &amp;= \log M_{X_1 - E[X_1]}(\lambda) + \log M_{X_2 - E[X_2]}(\lambda) \\         &amp;\leq \sum_{i=1}^{2} \frac{\lambda \sigma_i^2}{2} \tag*{\textcolor{purple}{\text{(sub-Gaussianity of $X_1$ and $X_2$)}}} \\         &amp;= \frac{\lambda^2}{2} \sum_{i=1}^{2} \sigma_i^2. \tag*{$[a_i, b_i]$} \end{align*}$$</div><p></p>
<p>Hence, <span class="mathjax-exps">$X_1 + X_2$</span> is sub-Gaussian with parameter <span class="mathjax-exps">$\sum_{i=1}^{2} \sigma_i^2$</span>.</p>
<p>Combining this result with the preceding theorem and Chernoff's bound for sub-Gaussian RVs gives <span class="mathjax-exps">$\textcolor{blue}{(2)}$</span> when <span class="mathjax-exps">$n=2$</span>.<br>
<br></p>
<h3 id="sub-exponential-random-variables">Sub-Exponential Random Variables </h3>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#42801f; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Definition.</strong> A random variable is called <span class="mathjax-exps">$\textcolor{blue}{\text{sub-exponential}}$</span> with parameters <span class="mathjax-exps">$(\sigma^2,b)$</span> if<br>
</p><div class="mathjax-exps">$$\log M_{X-\mu} (\lambda) \leq \frac{\lambda^2 \sigma^2}{2}. \tag*{\textcolor{gray}{$E[\exp\{ \lambda(X-\mu) \}]$}}$$</div><p></p>
</div>
<br>
<p><strong>Note:</strong> A sub-Gaussian random variable with parameter <span class="mathjax-exps">$\sigma^2$</span> is sub-exponential with parameters <span class="mathjax-exps">$(\sigma^2,b)$</span> for <u>any</u> <span class="mathjax-exps">$b&gt;0$</span>.<br>
<br></p>
<h4 id="an-alternative-characterization-of-sub-exponential-random-variables">An alternative characterization of sub-exponential random variables </h4>
<p>A random variable <span class="mathjax-exps">$X$</span> is sub-exponential if any only if there exist constants <span class="mathjax-exps">$c&gt;0$</span>and <span class="mathjax-exps">$\ell&gt;0$</span> such that<br>
</p><div class="mathjax-exps">$$P \left\{ |X-\mu| \geq t \right\} \leq c P \left\{ \varepsilon_\ell \geq t \right\} \hspace{2em} \text{for all }t &gt;0,$$</div><p></p>
<p>where <span class="mathjax-exps">$\varepsilon_\ell \sim \text{Exp}(\ell)$</span>, so that <span class="mathjax-exps">$P\{ \varepsilon_\ell \geq t \} = e^{-\ell t}$</span>.</p>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#43b7d1; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Theorem.</strong> If <span class="mathjax-exps">$X$</span> is sub-exponential with parameters <span class="mathjax-exps">$(\sigma^2,b)$</span>, then, for all <span class="mathjax-exps">$t\geq 0$</span>,<br>
</p><div class="mathjax-exps">$$\log P \{ X-\mu \geq t \} \leq \begin{cases}     -\frac{t^2}{2\sigma^2}, &amp; \text{if } 0 \leq t \leq \frac{\sigma^2}{b}, \\     - \frac{t}{2b}, &amp; \text{if } t&gt;\frac{\sigma^2}{b}. \end{cases}$$</div><p></p>
<br>
<p><img src="chapter3\figures\IMG_0381.jpg" alt="plot of sub-E vs. Hoeffding"></p>
<br>
<p><strong>Proof.</strong> By Chernoff,<br>
</p><div class="mathjax-exps">$$\log P \{ X \geq \mu + t \} \leq - \sup_{\lambda\in(0,\frac{1}{b})} \underbrace{\left[\lambda t - \frac{\lambda^2 \sigma^2}{2}\right]}_{\textcolor{blue}{=: f(\lambda)}}. \tag*{\textcolor{green}{(3)}}$$</div><p></p>
<p>If we extend the definition of <span class="mathjax-exps">$f$</span> to the whole real line, we'd find that <span class="mathjax-exps">$f$</span> achieves its maximum at <span class="mathjax-exps">$\lambda^* = \frac{t}{\sigma^2}$</span>.</p>
<p>[image]</p>
<p>If <span class="mathjax-exps">$t&lt;\frac{\sigma^2}{b}$</span>, then <span class="mathjax-exps">$\lambda^* &lt;\frac{1}{b}$</span>, and so<br>
</p><div class="mathjax-exps">$$\sup_{\lambda\in(0,\frac{1}{b})} f(\lambda) = f(\lambda^*) = \lambda^* t - \frac{(\lambda^*)^2 \sigma^2}{2} = \frac{t^2 \sigma^2}{2}.$$</div><p></p>
<p>Plugging this into <span class="mathjax-exps">$\textcolor{green}{(3)}$</span> gives the find bounds on the theorem statement.</p>
<p>If <span class="mathjax-exps">$t\geq \frac{\sigma^2}{b}$</span>, then <span class="mathjax-exps">$\lambda^* \geq \frac{1}{b}$</span>. Using that <span class="mathjax-exps">$f$</span> is monotone increasing on <span class="mathjax-exps">$(0,\frac{1}{b})$</span> (see picture above), it holds that<br>
</p><div class="mathjax-exps">$$\sup_{\lambda\in(0,\frac{1}{b})} f(\lambda) \stackrel{\scriptsize{\substack{\textcolor{blue}{(\text{cont.}} \\ \textcolor{blue}{\text{of }f)}}}}{=} f(\tfrac{1}{b}) = \frac{t}{b} - \frac{\sigma^2}{2b^2} \stackrel{\textcolor{blue}{(\tfrac{\sigma^2}{b} \leq t)}}{\geq} \frac{t}{2b}.$$</div><p></p>
<p>Combining this with <span class="mathjax-exps">$\textcolor{green}{(3)}$</span> gives us the second bound.</p>
</div>
<br>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#0000ff; padding:0.75em; padding-bottom: 0em;">
<p><big><strong>Example:</strong> Bounded random variable.</big></p>
<p>Suppose <span class="mathjax-exps">$X$</span> is bounded, so that <span class="mathjax-exps">$|X-\mu| \leq b$</span> a.s. for some <span class="mathjax-exps">$b&gt;0$</span>. Also, let <span class="mathjax-exps">$\sigma^2 := var(X)$</span>. We'll now show that <span class="mathjax-exps">$X$</span> is sub-exponential with parameters <span class="mathjax-exps">$(2\sigma^2, 2b)$</span>.</p>
<p><small><span class="mathjax-exps">$\textcolor{blue}{\text{From earlier: we know } X \text{ is sub-Gaussian with parameter } b^2}$</span>.</small></p>
<p>We'll show this as follows:<br>
</p><div class="mathjax-exps">$$\begin{align*}     M_{X-\mu}(\lambda) &amp;= E \left[ \exp \{ \lambda(X-\mu) \} \right] \\         &amp;= 1 + \frac{\lambda^2 \sigma^2}{2} + \sum_{k=3}^{\infty} \lambda^k \frac{E[(X-\mu)^k]}{k!} \\         &amp;= 1 + \frac{\lambda^2 \sigma^2}{2} + \sum_{k=3}^{\infty} \lambda^k \frac{E[(X-\mu)^2 \overbrace{(X-\mu)^{k-2}}^{\textcolor{purple}{|\,\cdot\,| \leq b^{k-2}}}]}{k!} \\         &amp;\leq 1 + \frac{\lambda^2 \sigma^2}{2} +\lambda^2\sigma^2 \sum_{k=3}^{\infty} \frac{|\lambda|^{k-2} b^{k-2}}{k!} \\         &amp;\leq 1 + \frac{\lambda^2 \sigma^2}{2} + \lambda^2\sigma^2 \sum_{k=3}^{\infty} (|\lambda| b)^{k-2} \\         &amp;= 1 + \frac{\lambda^2 \sigma^2}{2} \sum_{k=0}^{\infty} (|\lambda| b)^k. \end{align*}$$</div><p></p>
<p>If <span class="mathjax-exps">$|\lambda| b &lt; 1$</span>, then the geometric series above converges, yielding<br>
</p><div class="mathjax-exps">$$\begin{align*}     &amp;= 1 + \frac{\lambda^2 \sigma^2}{2 (1 - |\lambda| b)} \tag*{$\textcolor{orange}{(4)}$} \\     &amp;\leq \exp \left[ \frac{\lambda^2\sigma^2}{2(1 - |\lambda|b)} \right]. \end{align*}$$</div><p></p>
<p>If <span class="mathjax-exps">$|\lambda|&lt; \frac{1}{2b}$</span>, then <span class="mathjax-exps">$1-|\lambda| b &gt; \frac{1}{2}$</span>, yielding<br>
</p><div class="mathjax-exps">$$\leq \exp [\lambda^2\sigma^2]$$</div><p></p>
<p>whenever <span class="mathjax-exps">$|\lambda| &lt; \frac{1}{2b}$</span>. Hence, <span class="mathjax-exps">$X$</span> is sub-exponential with parameters <span class="mathjax-exps">$(2\sigma^2, 2b)$</span>.</p>
</div>
<br>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#43b7d1; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Theorem (Bernstein).</strong> Suppose <span class="mathjax-exps">$X$</span> is a random variable that is such that <span class="mathjax-exps">$|X - \mu| \leq b$</span> a.s. Let <span class="mathjax-exps">$\sigma^2:= var(X)$</span>. For all <span class="mathjax-exps">$t&gt;0$</span>,<br>
</p><div class="mathjax-exps">$$P \left\{ |X-\mu| \geq t \right\} \leq \exp \left[ - \frac{t^2}{2\{ \sigma^2 + bt \}} \right].$$</div><p></p>
<br>
<p><strong>Proof.</strong> By Chernoff and <span class="mathjax-exps">$\textcolor{orange}{\text{(4)}}$</span>, we have, for any <span class="mathjax-exps">$\lambda\in[0,\frac{1}{b})$</span>,<br>
</p><div class="mathjax-exps">$$\begin{align*}     \log P \{ X-\mu \geq t \} &amp;\leq - [\lambda t  - \log M_{X-\mu}(\lambda)] \\         &amp;\leq - \left[ \lambda t + \frac{\lambda^2 \sigma^2}{2 (1-|\lambda|b)} \right]. \end{align*}$$</div><p></p>
<p>Plugging in <span class="mathjax-exps">$\lambda = \frac{t}{bt+\sigma^2}\in[0,\frac{1}{b})$</span> into the above given the result.</p>
</div>
<br>
<p>Similar calculations can be used to establish the following more general form of Bernstein's inequality.</p>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#43b7d1; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Theorem (Bernstein).</strong> If <span class="mathjax-exps">$X_1,\dots,X_n$</span> are independent random variables satisfying <span class="mathjax-exps">$|X_i - \mu_i | \leq b$</span> a.s. for all <span class="mathjax-exps">$i$</span>. Let <span class="mathjax-exps">$\sigma_i^2 := var(X_i)$</span> and <span class="mathjax-exps">$\bar{\sigma}^2_n := \frac{1}{n} \sum_{i=1}^{n} \sigma_i^2$</span>. For all <span class="mathjax-exps">$t&gt;0$</span>,<br>
</p><div class="mathjax-exps">$$P \left\{ \bar{X}_n - E[\bar{X}_n] \geq t \right\} \leq \exp \left[ - \frac{nt^2}{\textcolor{blue}{2[\bar{\sigma}^2_n + bt]}} \right].$$</div><p></p>
</div>
<br>
<p>Comparing this to Hoeffding:<br>
</p><div class="mathjax-exps">$$\begin{align*}     P \left\{ \bar{X}_n - E[\bar{X}_n] \geq t \right\} \leq \exp \left[ - \frac{nt^2}{\textcolor{blue}{2b^2}} \right]. \end{align*}$$</div><p></p>
<p>When comparing these two bounds, it is enough to simply compare the denominators shown in blue.</p>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#000000; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><big><strong>Some Notes:</strong></big></p>
<ul>
<li>Both inequalities suggest that deviations from the mean of larger than <span class="mathjax-exps">$t=\frac{1}{\sqrt{n}}$</span> are unlikely.<br>
<br></li>
<li><u>But:</u> If <span class="mathjax-exps">$\bar{\sigma}_n^2$</span> is small, then Bernstein's inequality can give much sharper bounds than Hoeffding.
<ul>
<li><u>Why?</u> Well, if <span class="mathjax-exps">$\bar{\sigma}_n^2$</span> and <span class="mathjax-exps">$t$</span> are both small, then the denominator in Bernstein's inequality is also small (<span class="mathjax-exps">$\textcolor{blue}{2[\bar{\sigma}_n^2 + bt]}$</span>) — and, in particular, much small than the denominator in Hoeffding's inequality (<span class="mathjax-exps">$\textcolor{blue}{2b^2}$</span>).</li>
</ul>
</li>
</ul>
</div>
<br>
<br>
<h3 id="bounded-differences-inequality">Bounded Differences Inequality </h3>
<ul>
<li>
<p>So far, we've been showing that a sample mean <span class="mathjax-exps">$\bar{X}_n$</span> is tightly concentrated about its mean <span class="mathjax-exps">$E[\bar{X}_n]$</span>.<br>
<br></p>
</li>
<li>
<p>Now, we'll turn to more general cases where we have <span class="mathjax-exps">$f: \mathcal{X}^n \to \mathbb{R}$</span> and wish to derive upper bounds on<br>
</p><div class="mathjax-exps">$$P \left\{ | f(X_1^n) - E[f(X_1^n)] | \geq t \right\}$$</div><p></p>
<p>(If we don't know anything about <span class="mathjax-exps">$f$</span>, we can't say anything here.)<br>
<br></p>
</li>
<li>
<p>A key example we'll study is the case where<br>
</p><div class="mathjax-exps">$$f\underbrace{(x_1,x_2,\dots,x_n)}_{x_1^n} := \sup_{g\in G} \left| \frac{1}{n} \sum_{i=1}^{n} \left\{ g(X_i) - E[g(X_i)] \right\} \right|.$$</div><p></p>
<p>E.g. <span class="mathjax-exps">$G = \left\{ x \mapsto \bold{1}[x\geq t]: t\in\mathbb{R} \right\}$</span>.<br>
<br></p>
</li>
<li>
<p>A function <span class="mathjax-exps">$f$</span> is said to satisfy the <span class="mathjax-exps">${\textcolor{blue}{\text{bounded differences property (BDP)}}}$</span> with parameters <span class="mathjax-exps">$c_1,c_2,\dots,c_n$</span> if, for all <span class="mathjax-exps">$i \in \{ 1,2,\dots,n \}$</span> and all <span class="mathjax-exps">$x_1,x_2,\dots,x_n, x_i' \in \mathcal{X}$</span>:<br>
</p><div class="mathjax-exps">$$\left| f(x_1,\dots,x_{i-1}, \textcolor{blue}{x_i}, x_{i+1}, \dots, x_n) - f(x_1, \dots, x_{i-1}, \textcolor{blue}{x_i'}, x_{i+1}, \dots, x_n) \right| \leq c_i.$$</div><p></p>
<p>(This is different than a Lipschitz condition. More about this at 26:00 in the Zoom lecture.)<br>
<br></p>
</li>
<li>
<p>This property enforces that <span class="mathjax-exps">$f$</span> can't depend too heavily on any one of its inputs.</p>
</li>
</ul>
<p>We now revisit our example.</p>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#0000ff; padding:0.75em; padding-bottom: 0em;">
<p><big><strong>Example:</strong> Bounded random variable (continued).</big></p>
<p></p><div class="mathjax-exps">$$f(x_1^n) := \sup_{g\in G} \left| \frac{1}{n} \sum_{i=1}^{n} \left\{ g(x_i) - E[g(X_i)] \right\} \right|.$$</div><p></p>
<p>We will require that the functions in <span class="mathjax-exps">$G$</span> are uniformly bounded, and, in particular,<br>
</p><div class="mathjax-exps">$$\sup_{g\in G} \sup_{x\in\mathcal{X}} | g(x) | \leq 1.$$</div><p></p>
<p>We'll show that <span class="mathjax-exps">$f$</span> satisfies the BDP with parameters <span class="mathjax-exps">$c_i := \frac{2}{n}$</span> for all <span class="mathjax-exps">$i$</span>.</p>
<p>To see this, we'll use that, by the triangle inequality,<br>
</p><div class="mathjax-exps">$$\sup_t | a_t | - \sup |b_t| \leq \sup_t |a_t - b_t|. \tag*{\textcolor{lightgreen}{($\bigstar$)}}$$</div><p></p>
<p>We have that<br>
</p><div class="mathjax-exps">$$\begin{align*}     f(x_1^n) &amp;- f(x_1,\dots,x_{i-1}, x_i', x_{i+1}, \dots, x_n) \\         &amp;= \sup_{g\in G} \left| \frac{1}{n} \sum_{j=1}^{n} \left\{ g(x_j) - E[g(X_j)] \right\} \right| \\         &amp;\hspace{4em}- \sup_{g\in G} \left| \frac{1}{n} \sum_{j=1}^{n} \{ g(x_j) - E[g(X_j)] \}  + \frac{1}{n} \left\{ g(x_i') - g(x_i) \right\} \right| \\         &amp;\leq \sup_{g\in G} \left| \frac{1}{n} \{ g(x_i) - g(x_i') \} \right| \tag*{by \textcolor{lightgreen}{($\bigstar$)}} \\         &amp;\leq \frac{2}{n} \tag{$g(x) \in [-1,1]$} \end{align*}$$</div><p></p>
<p>By symmetry, we have <span class="mathjax-exps">$|f(x_1^n) - f(x_1,\dots,x_{i-1}, x_i', x_{i+1},\dots, x_n) \leq \frac{2}{n}$</span>. Hence, the BDP holds with <span class="mathjax-exps">$c_i = \frac{2}{n}$</span>.</p>
</div>
<br>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#43b7d1; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Theorem (Bounded differences inequality; McDiarmid's inequality).</strong> If <span class="mathjax-exps">$X_1,\dots,X_n$</span> are independent RVs and <span class="mathjax-exps">$f$</span> satisfies the BDP with parameters <span class="mathjax-exps">$c_1,\dots,c_n$</span>, then, for all <span class="mathjax-exps">$t&gt;0$</span>,<br>
</p><div class="mathjax-exps">$$\textcolor{blue}{P \left\{ | f(X_1^n) - E[f(X_1^n)] | \geq t \right\}} \leq 2 \exp \left\{ - \frac{2t^2}{\sum_{i=1}^{n} c_i^2} \right\}.$$</div><p></p>
<br>
<p><strong>Proof.</strong> Let<br>
</p><div class="mathjax-exps">$$\begin{align*}     D_1 := E[f(X_1^n) | X_1] - E[f(X)] \\     D_j := E[f(X_1^n) | X_1^j] - E[f(X_1^n) | X_1^{j-1}], \hspace{1em} j=2,3,\dots,n  \end{align*}$$</div><p></p>
<p>where <span class="mathjax-exps">$X_1^j = \textcolor{blue}{X_1,X_2,\dots,X_j}$</span>.</p>
<p>Note that<br>
</p><div class="mathjax-exps">$$\textcolor{blue}{f(X_1^n) - E[f(X_1^n)]} = \textcolor{green}{\sum_{j=1}^{n} D_j} \tag*{\textcolor{red}{(telescoping sum)}}.$$</div><p></p>
<p>Thus, to establish our desired result, it's enough to show that<br>
</p><div class="mathjax-exps">$$P \left\{ \left| \textcolor{green}{\sum_{j=1}^{n} D_j} \right| \geq t\right\} \leq 2 \exp \left\{ -\frac{2t^2}{\sum_{j=1}^{n} c_j^2} \right\}. \tag{6}$$</div><p></p>
<p><strong>Note:</strong> <span class="mathjax-exps">$D_j$</span> satisfies the following properties:</p>
<ol>
<li>
<p><span class="mathjax-exps">$D_j$</span> is a function of <span class="mathjax-exps">$(X_1,\dots, X_j)$</span> and is independent of <span class="mathjax-exps">$(X_{j+1}, \dots, X_n)$</span>;</p>
</li>
<li>
<p><span class="mathjax-exps">$E[D_j | X_1^{j-1}] = E[E[f(X_1^n) | X_1^j] - E[f(X_1^n) | X_1^{j-1}] X_1^{j-1} |] = 0$</span>;</p>
</li>
<li>
<p><span class="mathjax-exps">$E|D_j|&lt;\infty$</span>.</p>
</li>
</ol>
<p>Hence, <span class="mathjax-exps">$\{ D_j \}_{j=1}^{n}$</span> is a martingale difference sequence with respect to the filtration generated by <span class="mathjax-exps">$X_1, \dots, X_j$</span>.</p>
</div>
<br>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#aa46e0; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Lemma (Azum-Hoeffding).</strong> <span class="mathjax-exps">$(6)$</span> holds.<br>
<br></p>
<p><strong>Proof.</strong> We'll repeatedly make use of the sub-Gaussianity of bounded random variables in this proof.</p>
<p>Towards the end of the proof, we'll have established that <span class="mathjax-exps">$\sum_{j=1}^{n} D_j$</span> is sub-Gaussian with parameter <span class="mathjax-exps">$\frac{\sum_{j=1}^{n} c_j^2}{4}$</span>.</p>
<p>To begin, note that <span class="mathjax-exps">$L_j \leq D_j \leq U_j$</span>, where<br>
</p><div class="mathjax-exps">$$\begin{align*}     L_j &amp;:= \inf_{x_j} E[f(X_1^n) | X_1,\dots,X_{j-1}, X_j = x_j] - E[f(X_1^n) | X_1^{j-1}], \\     U_j &amp;:= \sup_{x_j} E[f(X_1^n) | X_1^{j-1}, X_j = x_j] - E[f(X_1^n) | X_1^{j-1}]. \end{align*}$$</div><p></p>
<p>Recalling that <span class="mathjax-exps">$-\inf_t a_t = \sup_t[-a_t]$</span>, we'll now use that <span class="mathjax-exps">$f$</span> satisfies the BDP to show that <span class="mathjax-exps">$U_j - L_j \leq c_j$</span>:<br>
</p><div class="mathjax-exps">$$\begin{align*} 	U_j - L_j &amp;= \sup_{x_j, x_j'} \{ E[f(X_1^n) | X_1^{j-1}, X_j=x_j] - E[f(X_1^n) | X_1^{j-1}, X_j=x_j'] \} \\     &amp;= \sup_{x_j, x_j'} \left\{ E \left[ f(X_1^{j-1}, x_j, X_{j+1}^n) | X_1^{j-1}, \cancel{X_j=x_j} \right] - E \left[ f(X_1^{j-1}, x_j, X_{j+1}^n) | X_1^{j-1}, \cancel{X_j=x_j'} \right] \right\} \\         &amp;\hspace{7em} \text{(due to independence)} \\         &amp;= \sup_{x_j, x_j'} E\left[ f(X_1^{j-1}, x_j, X_{j+1}^n) - f(X_1^{j-1}, x_j', X_{j+1}^n) \mid X_1^{j-1} \right] \\         &amp;\leq c_j. \tag{BDP} \end{align*}$$</div><p></p>
<p>Note that through all of this, <span class="mathjax-exps">$x_j, x_j' \in \mathcal{X}$</span>.</p>
<p>Noting that <span class="mathjax-exps">$L_j \leq D_j \leq U_j$</span> a.s. and also that <span class="mathjax-exps">$L_j$</span> and <span class="mathjax-exps">$U_j$</span> are only random through <span class="mathjax-exps">$X_1^{j-1}$</span>, we see that <span class="mathjax-exps">$D_j | X_1^{j-1}$</span> is sub-Gaussian with parameter <span class="mathjax-exps">$\sigma_j^2 = \frac{c_j^2}{4}$</span>, in the sense that<br>
</p><div class="mathjax-exps">$$E\left[ \exp(\lambda D_j) | X_1^{j-1} \right] \leq \exp \left( \frac{\lambda^2 c_j^2}{8} \right). \tag{$\bigstar$}$$</div><p></p>
<p>Applying this result when <span class="mathjax-exps">$j=n$</span> yields that<br>
</p><div class="mathjax-exps">$$\begin{align*}     E \left[ \exp \left( \lambda \textcolor{green}{\sum_{j=1}^{n} D_j} \right) \right] &amp;= E \left[ E \left( \exp \left\{ \lambda \left. \sum_{j=1}^{n} D_j \right\} \right| X_1^{n-1} \right) \right] \\         &amp;= E \left[ \exp \left\{ \lambda \sum_{j=1}^{n} D_j \right\} E \left( \exp \{ \lambda D_n \} | X_1^{n-1} \right) \right] \\         &amp;\leq \exp \left( \frac{\lambda^2 c_n^2}{8} \right) E \left[ \exp \left\{ \lambda \sum_{j=1}^{n-1} D_j \right\} \right]. \end{align*}$$</div><p></p>
<p>Iterating this argument shows that<br>
</p><div class="mathjax-exps">$$E \left[ \exp \left( \lambda \textcolor{green}{\sum_{j=1}^{n} D_j} \right) \right] \leq \exp \left( \frac{\lambda^2 \sum_{j=1}^{n} c_j^2}{8} \right).$$</div><p></p>
<p>Hence, we've shown that <span class="mathjax-exps">$\sum_{j=1}^{n} D_j$</span> is sub-Gaussian with parameter <span class="mathjax-exps">$\frac{\sum_{j=1}^{n} c_j^2}{4}$</span>. The result in the lemma follows by applying Chernoff twice, once to bound the probability of <span class="mathjax-exps">$\left\{ \textcolor{green}{\sum_{j=1}^{n} D_j} \geq t \right\}$</span> and again to bound that of <span class="mathjax-exps">$\left\{ \textcolor{green}{\sum_{j=1}^{n} D_j} \geq -t \right\}$</span>.</p>
</div>
<br>
<p></p>
<br>
<p style="border-spacing:0px; page-break-before: always">
</p><h2 id="uchapter-4-part-1u"><u>Chapter 4: Part 1.</u> </h2>
<p><big><strong>Chapter 4, Part 1 Table of Contents</strong></big></p>
<ul>
<li><a href="chapter4\part1\chapter4-part1.md#empirical-risk-minimization">Empirical Risk Minimization</a></li>
<li><a href="chapter4\part1\chapter4-part1.md#relation-to-m-estimation">Relation to M-estimation</a></li>
<li><a href="chapter4\part1\chapter4-part1.md#rademacher-complexity">Rademacher Complexity</a>
<ul>
<li><a href="chapter4\part1\chapter4-part1.md#symmetrization-argument-to-bound-evert-p_n---pvert_mathcalf">Symmetrization argument to bound <span class="mathjax-exps">$E\Vert P_n - P\Vert\_\mathcal{F}$</span></a></li>
<li><a href="chapter4\part1\chapter4-part1.md#almost-matching-lower-bound-via-desymmetrization">Almost-matching lower bound via desymmetrization</a></li>
</ul>
</li>
<li><a href="chapter4\part1\chapter4-part1.md#vapnik-chervonenkis-vc-dimension">Vapnik-Chervonenkis (VC) dimension</a></li>
<li><a href="chapter4\part1\chapter4-part1.md#quick-detour-bracketing-and-covering-numbers">Quick Detour: Bracketing and Covering Numbers.</a>
<ul>
<li><a href="chapter4\part1\chapter4-part1.md#varepsilon-bracketing-numbers"><span class="mathjax-exps">$\varepsilon$</span>-bracketing Numbers</a></li>
<li><a href="#covering-numbers">Covering Numbers</a></li>
</ul>
</li>
</ul>
<p><strong>Other Sections:</strong> <a href="#uchapter-1u-minimaxity">Chapter 1</a>, <a href="#uchapter-2u-kernel-density-estimation">Chapter 2</a>, <a href="#uchapter-3u-concentration-inequality">Chapter 3</a>, <a href="#uchapter-4-part-2u">Chapter 4 - Part 2</a></p>
<br>
<p></p><ul>
<li><strong>Motivation:</strong> empirical risk minimization</li>
<li>Rademacher complexity</li>
<li>VC dimension</li>
</ul>
<h3 id="empirical-risk-minimization">Empirical Risk Minimization </h3>
<ul>
<li>
<p>Suppose that <span class="mathjax-exps">$X_1,X_2,\dots,X_n\stackrel{iid}{\sim} P$</span></p>
<ul>
<li>We'll let <span class="mathjax-exps">$X$</span> denote a generic draw from <span class="mathjax-exps">$P$</span></li>
</ul>
</li>
<li>
<p>Note that <span class="mathjax-exps">$Pf:= E_P[f(X)]$</span> and <span class="mathjax-exps">$P_n f := \frac{1}{n} \sum_{i=1}^{n} f(X_i)$</span></p>
</li>
<li>
<p>We'll let <span class="mathjax-exps">$\ell: \mathcal{X} \times \Theta \to \mathbb{R}$</span> and we'll assume that smaller values of the <span class="mathjax-exps">$\textcolor{blue}{\text{loss}}$</span> are preferable.</p>
</li>
<li>
<p><strong>Goal:</strong> Find a <span class="mathjax-exps">$\hat{\theta} \in\Theta$</span> such that the <span class="mathjax-exps">$\textcolor{blue}{\underline{\text{risk}}}$</span><br>
</p><div class="mathjax-exps">$$\int \ell(x,\hat{\theta}) dP(x) =: P\ell(\,\cdot\,, \hat{\theta})$$</div><p></p>
<p>that is nearly equal to<br>
</p><div class="mathjax-exps">$$\inf_{\theta\in\Theta} P\ell(\,\cdot\,,\theta).$$</div><p></p>
<p>To ease notation, I'll suppose the infimum is achieved at some <span class="mathjax-exps">$\theta_0\in\Theta$</span>. Also, we'll write <span class="mathjax-exps">$\ell(\theta)$</span> to denote <span class="mathjax-exps">$x \mapsto \ell(x,\theta)$</span>.</p>
</li>
<li>
<p>To quantify how close we are to achieving our goal, we'll consider the <span class="mathjax-exps">$\textcolor{blue}{\underline{\text{regret}}}$</span>:<br>
</p><div class="mathjax-exps">$$\text{Reg}(\hat{\theta}) := P \ell(\hat{\theta}) - \underbrace{P\ell(\theta_0)}_{\scriptsize{=\displaystyle\inf_{\theta\in\Theta} P\ell(\theta)}}.$$</div><p></p>
<p><strong>Note:</strong> <span class="mathjax-exps">$\text{Reg}(\hat{\theta}) \geq 0$</span> and <span class="mathjax-exps">$\text{Reg}(\theta_0)=0$</span>.</p>
</li>
</ul>
<br>
<p><big><strong>How can we construct <span class="mathjax-exps">$\hat{\theta}?$</span></strong></big><br>
If we had access to <span class="mathjax-exps">$P$</span>, then we can find <span class="mathjax-exps">$\theta_0$</span> by simply solving<br>
</p><div class="mathjax-exps">$$\text{minimize}\; \textcolor{blue}{P}\ell(\theta) \\ \text{subject to } \theta\in\Theta.$$</div><p></p>
<p>Since we don't know <span class="mathjax-exps">$P$</span>, in practice we'll replace it by an estimate: the empirical distribution <span class="mathjax-exps">$P_n$</span>. So, our <span class="mathjax-exps">$\textcolor{blue}{\underline{\text{empirical risk minimizer}}}$</span> <span class="mathjax-exps">$\hat{\theta}$</span> is a solution to<br>
</p><div class="mathjax-exps">$$\text{minimize}\; \textcolor{blue}{P_n}\ell(\theta) \\ \text{subject to } \theta\in\Theta,$$</div><p></p>
<p>where <span class="mathjax-exps">${P_n}\ell(\theta)$</span> is called the <span class="mathjax-exps">$\textcolor{blue}{\text{``empirical risk"}}$</span>.</p>
<br>
<h3 id="relation-to-m-estimation">Relation to M-estimation </h3>
<ul>
<li>Empirical risk minimization is a special case of M-estimation, which we say in STAT 581.</li>
<li>Often, when people use the term <span class="mathjax-exps">$\textcolor{blue}{\text{``M-estimation"}}$</span> the objective is to establish a <span class="mathjax-exps">$\textcolor{magenta}{\text{weak convergence}}$</span> result for <span class="mathjax-exps">$\hat{\theta}$</span>.
<ul>
<li>Something like<br>
<div class="mathjax-exps">$$\sqrt{n} (\hat{\theta}- \theta_0) \stackrel{d}{\to} Z.$$</div></li>
</ul>
</li>
<li>In contrast, when people use the term <span class="mathjax-exps">$\textcolor{blue}{\text{``empirical risk minimization"}}$</span>, the goal is to obtain a <span class="mathjax-exps">$\hat{\theta}$</span> with <span class="mathjax-exps">$\textcolor{magenta}{\text{small regret}}$</span> (<span class="mathjax-exps">$o_p(n^{-2/3})$</span>, for example(?)).</li>
</ul>
<br>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#0000ff; padding:0.75em; padding-bottom: 0em;">
<p><big><strong>Example:</strong> Regression function.</big></p>
<ul>
<li>
<p>Suppose <span class="mathjax-exps">$X=(W,Y)$</span>, where <span class="mathjax-exps">$W$</span> is a feature and <span class="mathjax-exps">$Y$</span> is a real-valued outcome.</p>
</li>
<li>
<p>The goal is to predict <span class="mathjax-exps">$Y$</span> based on <span class="mathjax-exps">$W$</span>.</p>
</li>
<li>
<p>Suppose <span class="mathjax-exps">$\Theta\subset L^2(P_W)$</span>, where <span class="mathjax-exps">$L^2(P_W)$</span> consists of all <span class="mathjax-exps">$f:\mathcal{W}\to\mathbb{R}$</span> satisfying <span class="mathjax-exps">$\int f(w)^2 d \underbrace{P_W}_{\substack{\textcolor{blue}{\text{marg.}} \\ \textcolor{blue}{\text{dist'n}}}}(w) &lt;\infty$</span>.</p>
</li>
<li>
<p>We'll use the squared error loss<br>
</p><div class="mathjax-exps">$$\ell(x,\theta):= [y-\theta(w)]^2,$$</div><p></p>
<p>where <span class="mathjax-exps">$x=(w,y)$</span></p>
</li>
<li>
<p>If <span class="mathjax-exps">$\Theta$</span> contains <span class="mathjax-exps">$w\mapsto E_P[Y|W=w]$</span>, then <span class="mathjax-exps">$\theta_0$</span> is this function. Otherwise, <span class="mathjax-exps">$\theta_0$</span> is the projection of <span class="mathjax-exps">$w\mapsto E_P[Y|W=w]$</span> onto <span class="mathjax-exps">$\Theta$</span>, in that<br>
</p><div class="mathjax-exps">$$\theta_0 := \argmin_{\theta\in\Theta} \int \left\{ \theta(w) - E_P[Y|W=w] \right\}^2 dP_W(w).$$</div><p></p>
</li>
</ul>
</div>
<br>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#0000ff; padding:0.75em; padding-bottom: 0em;">
<p><big><strong>Example:</strong> Classification.</big></p>
<ul>
<li>
<p>Same as the previous setting, except <span class="mathjax-exps">$Y\in\{ 0,1 \}$</span> and the goal is to <span class="mathjax-exps">$\textcolor{blue}{\text{classify}}$</span> <span class="mathjax-exps">$Y$</span> based on <span class="mathjax-exps">$W$</span>.</p>
<ul>
<li><span class="mathjax-exps">$\Theta$</span> consists of maps from <span class="mathjax-exps">$\mathcal{W}$</span> to <span class="mathjax-exps">$\{ 0,1 \}$</span>.</li>
</ul>
</li>
<li>
<p>Consider the 0-1 loss:<br>
</p><div class="mathjax-exps">$$\ell(x,\theta) = \bold{1}[\theta(w) \neq y].$$</div><p></p>
</li>
<li>
<p>If <span class="mathjax-exps">$\Theta$</span> contains <span class="mathjax-exps">$w\mapsto \bold{1}\{ E_P[Y|W=w] &gt; \frac{1}{2} \}$</span>, then <span class="mathjax-exps">$\theta_0$</span> can be taken to be equal to this function and<br>
</p><div class="mathjax-exps">$$\text{Reg}(\theta) = E_P [|2E_P[Y|W] - 1| \mid \bold{1} \{ \theta(W) \neq \theta_0(W) \}].$$</div><p></p>
</li>
</ul>
</div>
<br>
<p>We now provide a template for analyzing<br>
</p><div class="mathjax-exps">$$\text{Reg}(\hat{\theta}) := P\ell(\hat{\theta}) - P\ell(\theta_0)$$</div><p></p>
<p>when <span class="mathjax-exps">$\hat{\theta}$</span> is an ERM.<br>
</p><div class="mathjax-exps">$$\begin{align*}     (0\leq ) \text{ Reg}(\hat{\theta}) &amp;:= P\ell(\hat{\theta}) - P\ell(\theta_0) \\         &amp;\leq P\ell(\hat{\theta}) - P\ell(\theta_0) + \underbrace{P_n \ell(\theta_0) - P_n\ell(\hat{\theta})}_{\textcolor{purple}{\geq 0}} \tag*{$\textcolor{purple}{(\hat{\theta} \text{ is an ERM})}$} \\         &amp;= (P_n - P) [\ell(\theta_0)-\ell(\hat{\theta})] \\         &amp;\leq \left|(P_n-P) \ell(\theta_0)\right| + \left|(P_n-P)\ell(\hat{\theta})\right| \tag*{$\textcolor{purple}{(\text{triangle ineq.})}$} \\         &amp;\leq 2\sup_{\theta\in\Theta} \left| (P_n - P) \ell(\theta) \right| \tag*{$\textcolor{purple}{(\theta_0, \hat{\theta} \in \Theta)}$} \\         &amp;= 2\sup_{f\in\mathcal{F}} |(P_n - P)f|, \end{align*}$$</div><p></p>
<p>where <span class="mathjax-exps">$\mathcal{F}:= \{ x\mapsto \ell(x,\theta) : \theta\in\Theta \}$</span>.</p>
<p>We've shown that<br>
</p><div class="mathjax-exps">$$0\leq \text{Reg}(\hat{\theta}) \leq 2 \underbrace{\sup_{f\in\mathcal{F}} |(P_n-P)f|}_{\textcolor{blue}{=: \Vert P_n-P \Vert_{\mathcal{F}}}}$$</div><p></p>
<br>
<h3 id="rademacher-complexity">Rademacher Complexity </h3>
<ul>
<li>
<p>Suppose <span class="mathjax-exps">$\mathcal{F}$</span> consists of <span class="mathjax-exps">$[0,1]$</span>-valued functions.</p>
<ul>
<li>From our example in Chapter 3, we know that <span class="mathjax-exps">$\Vert P_n - P \Vert_{\mathcal{F}}$</span> satisfies the BDP with parameters <span class="mathjax-exps">$c_i = \frac{1}{n}$</span>, <span class="mathjax-exps">$i\in\{1,2,\dots,n\}$</span>.</li>
</ul>
</li>
<li>
<p>Hence, by McDiarmid's inequality, for all <span class="mathjax-exps">$t&gt;0$</span>,<br>
</p><div class="mathjax-exps">$$P \left\{ \left| \Vert P_n - P\Vert_\mathcal{F} - E\Vert P_n - P\Vert_{\mathcal{F}} \right|\geq t \right\} \leq 2 \exp \{ -2nt^2 \}.$$</div><p></p>
<p>So, with high probability, <span class="mathjax-exps">$\Vert P_n - P\Vert_\mathcal{F}$</span> is close to its mean!</p>
<ul>
<li>To get a high-probability bound on <span class="mathjax-exps">$\Vert P_n - P \Vert_{\mathcal{F}}$</span>, it's enuogh to get a high-probability bound on <span class="mathjax-exps">$E\Vert P_n - P \Vert_\mathcal{F}$</span>.<br>
<br></li>
</ul>
</li>
</ul>
<h4 id="symmetrization-argument-to-bound-evert-p_n---pvert_mathcalf">Symmetrization argument to bound <span class="mathjax-exps">$E\Vert P_n - P\Vert_\mathcal{F}$</span> </h4>
<ul>
<li>Let <span class="mathjax-exps">$X_1,X_2,\dots,X_n, X_1', X_2', \dots, X_n'\stackrel{iid}{\sim} P$</span>.
<ul>
<li><span class="mathjax-exps">$X_1,X_2,\dots,X_n$</span> is our original sample</li>
<li><span class="mathjax-exps">$X_1', X_2', \dots, X_n'$</span> is called a <span class="mathjax-exps">$\textcolor{blue}{\text{ghost sample}}$</span></li>
</ul>
</li>
<li>Let <span class="mathjax-exps">$P_n'$</span> denote the empirical distribution of <span class="mathjax-exps">$X_1',X_2',\dots,X_n'$</span>.</li>
</ul>
<p>We have that<br>
</p><div class="mathjax-exps">$$\begin{align*}     E\Vert P_n - P \Vert_{\mathcal{F}} &amp;= E \left[ \sup_{f\in\mathcal{F}} \left| \frac{1}{n} \sum_{i=1}^{n} f(X_i) - E_P[f(X_i')] \right| \right] \\         &amp;= E \left\{ \left| \sup_{f\in\mathcal{F}} E \left[ \frac{1}{n} \sum_{i=1}^{n} \{ f(X_i) - f(X_i') \} | X_1^n \right] \right| \right\} \tag*{\textcolor{purple}{(\text{linearity of expectation})}} \\         &amp;\leq E \left\{ E \left[ \sup_{f\in\mathcal{F}} \left| \frac{1}{n} \sum_{i=1}^{n} \{ f(X_i) - f(X_i') \} \right| X_1^n \right] \right\} \\         &amp;= E \left[ \sup_{f\in\mathcal{F}} \left| \frac{1}{n} \sum_{i=1}^{n} \{ f(X_i) - f(X_i') \} \right| \right]. \end{align*}$$</div><p></p>
<p>Let <span class="mathjax-exps">$\varepsilon_1,\dots,\varepsilon_n \stackrel{iid}{\sim} \text{Rademacher}$</span> that are independent <span class="mathjax-exps">$X_1,X_2,\dots,X_n, X_1',X_2',\dots,X_n'$</span>. Since <span class="mathjax-exps">$X_1,X_2,\dots,X_n,X_1',X_2',\dots,X_n'\stackrel{iid}{\sim} P$</span>, we can continue the above as<br>
</p><div class="mathjax-exps">$$\begin{align*}     &amp;= E \left[ \sup_{f\in\mathcal{F}} \left| \frac{1}{n} \sum_{i=1}^{n} \varepsilon_i \left\{ f(X_i) - f(X_i') \right\} \right| \right] \\     &amp;\leq E \left[ \sup_{f\in\mathcal{F}} \left| \frac{1}{n} \sum_{i=1}^{n} \varepsilon_i f(X_i) \right| \right] + E \left[ \sup_{f\in\mathcal{F}} \left| \frac{1}{n} \sum_{i=1}^{n} \varepsilon_i f(X_i') \right| \right]\\     &amp;= 2 E \left[ \sup_{f\in\mathcal{F}} \left| \frac{1}{n} \sum_{i=1}^{n} \varepsilon_i f(X_i) \right| \right]. \end{align*}$$</div><p></p>
<p>To write this more cleanly, we'll define the <span class="mathjax-exps">$\textcolor{blue}{\text{Rademacher process}}$</span> <span class="mathjax-exps">$R_n: \mathcal{F}\to\mathbb{R}$</span> as<br>
</p><div class="mathjax-exps">$$R_n(f) := \frac{1}{n} \sum_{i=1}^{n} \varepsilon_i f(X_i).$$</div><p></p>
<p>Also let <span class="mathjax-exps">$\Vert R_n \Vert_\mathcal{F} := \sup_{f\in\mathcal{F}} |R_n(f)|$</span>. Hence, we've shown that<br>
</p><div class="mathjax-exps">$$\underbrace{E\Vert P_n -P \Vert_{\mathcal{F}}}_{\text{``empirical process term"}} \leq 2 \underbrace{E \Vert R_n \Vert_{\mathcal{F}}}_{\substack{\text{the ``Rademacher} \\ \text{complexity of } \mathcal{F} \;\!"}}.$$</div><p></p>
<p>Often, when studying <span class="mathjax-exps">$E\Vert R_n \Vert_{\mathcal{F}}$</span>, we use that<br>
</p><div class="mathjax-exps">$$E\Vert R_n \Vert_\mathcal{F} = E \underbrace{\left\{ {E [\Vert R_n \Vert_\mathcal{F} | X_1^n]} \right\}}_{\textcolor{blue}{\substack{\text{``empirical Rademacher} \\ \text{complexity"}}}}.$$</div><p></p>
<br>
<h4 id="almost-matching-lower-bound-via-desymmetrization">Almost-matching lower bound via desymmetrization </h4>
<p></p><div class="mathjax-exps">$$E \Vert P_n - P \Vert_\mathcal{F} \geq \frac{1}{2} E \Vert R_n \Vert_\mathcal{F} - \sqrt{\frac{\log(2)}{2n}}.$$</div><p></p>
<p>Putting together what we've seen today gives the following theorem.</p>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#43b7d1; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Theorem.</strong> If <span class="mathjax-exps">$\mathcal{F}$</span> is a collection of <span class="mathjax-exps">$[0,1]$</span>-valued functions, then, with probability at least <span class="mathjax-exps">$1-2\exp (-2nt^2)$</span>,<br>
</p><div class="mathjax-exps">$$\begin{align*}         \frac{1}{2} E \Vert R_n \Vert_\mathcal{F} - \sqrt{\frac{\log(2)}{2n}} &amp;\leq E\Vert P_n-P \Vert_\mathcal{F} - t \\         &amp;\leq \Vert P_n - P \Vert_\mathcal{F} \tag{$(P_n-P)f = O_p(1/\sqrt{n})$} \\         &amp;\leq E\Vert P_n - P \Vert_\mathcal{F} + t \\         &amp;\leq 2 E\Vert P_n - P \Vert_\mathcal{F} + t. \end{align*}$$</div><p></p>
</div>
<br>
<br>
<h3 id="vapnik-chervonenkis-vc-dimension">Vapnik-Chervonenkis (VC) dimension </h3>
<ul>
<li>
<p>Let <span class="mathjax-exps">$\mathcal{F}$</span> be a class of functions mapping from <span class="mathjax-exps">$\mathcal{X}$</span> to <span class="mathjax-exps">$\{ 0,1 \}$</span>.</p>
</li>
<li>
<p>For <span class="mathjax-exps">$x_1^n = (x_1,x_2,\dots,x_n) \in\mathcal{X}^n$</span>, define the <span class="mathjax-exps">$\textcolor{blue}{\text{projection of } \mathcal{F} \text{ onto } x_1^n}$</span> as<br>
</p><div class="mathjax-exps">$$\mathcal{F}_{x_1^n} := \left\{ (f(x_1), f(x_2),\dots,f(x_n)): f\in\mathcal{F} \right\}.$$</div><p></p>
<ul>
<li>Hence, <span class="mathjax-exps">$|\mathcal{F}_{x_1^n}|\leq 2^n$</span></li>
<li>We say that <span class="mathjax-exps">$\mathcal{F}$</span> <span class="mathjax-exps">$\textcolor{blue}{\text{shatters}}$</span> <span class="mathjax-exps">$x_1^n$</span> if <span class="mathjax-exps">$|\mathcal{F}_{x_1^n}|=2^n$</span>.</li>
</ul>
</li>
<li>
<p>Define the <span class="mathjax-exps">$\textcolor{blue}{\text{growth function}}$</span> of <span class="mathjax-exps">$\mathcal{F}$</span> as<br>
</p><div class="mathjax-exps">$$\Pi_\mathcal{F}(n) := \sup_{x_1^n\in\mathcal{X}^n} | \mathcal{F}_{x_1^n}|.$$</div><p></p>
</li>
<li>
<p>The <span class="mathjax-exps">$\textcolor{blue}{\text{VC dimension}}$</span> of <span class="mathjax-exps">$\mathcal{F}$</span> is defined as<br>
</p><div class="mathjax-exps">$$VC(\mathcal{F}) := \sup \left\{ n\in\mathbb{N}: \Pi_{\mathcal{F}}(n) = 2^n \right\},$$</div><p></p>
<p>where the right hand side represents:<br>
</p><div class="mathjax-exps">$$\textcolor{orange}{\text{``the largest } n \text{ such that there exists a tuple } x_1^n \text{ that is shattered by } \mathcal{F}}".$$</div><p></p>
<ul>
<li>It's possible that <span class="mathjax-exps">$VC(\mathcal{F}) = +\infty$</span>.</li>
<li>The <span class="mathjax-exps">$\textcolor{blue}{\text{VC index}}$</span> of <span class="mathjax-exps">$\mathcal{F}$</span> is defined as<br>
<div class="mathjax-exps">$$VC(\mathcal{F})+1.$$</div></li>
</ul>
<p>Why will studying the VC dimension by useful?</p>
<ul>
<li>Well, we're trying to study<br>
<div class="mathjax-exps">$$\Vert P_n - P\Vert_\mathcal{F} := \sup_{f\in\mathcal{F}} \left| \frac{1}{n} \sum_{i=1}^{n} f(X_i) - Pf \right|$$</div></li>
</ul>
</li>
</ul>
<p>We can also define the VC dimension (or VC index) of a collection of sets.</p>
<ul>
<li>
<p>Let <span class="mathjax-exps">$\mathcal{A}$</span> denote the collection of subsets of <span class="mathjax-exps">$\mathcal{X}$</span>. Define<br>
</p><div class="mathjax-exps">$$\mathcal{F}_\mathcal{A} = \left\{ x\mapsto \bold{1}_B(x): B\in\mathcal{A} \right\}.$$</div><p></p>
<p>We then let<br>
</p><div class="mathjax-exps">$$VC(\mathcal{A}) := VC(\mathcal{F}_\mathcal{A}) \\ \Pi_\mathcal{A}(n) := \Pi_{\mathcal{F}_\mathcal{A}}(n).$$</div><p></p>
</li>
</ul>
<p>Suppose now that <span class="mathjax-exps">$\mathcal{F}$</span> consists of <span class="mathjax-exps">$\mathcal{X}\to\mathbb{R}$</span> functions. The VC dimension of <span class="mathjax-exps">$\mathcal{F}$</span> is defined as the VC dimension of the class of subgraphs <span class="mathjax-exps">$\mathcal{A}$</span>:<br>
</p><div class="mathjax-exps">$$\mathcal{A} := \left\{ {\{ (x,t)\in\mathcal{X}\times\mathbb{R}: t&lt; f(x)\}}: f\in\mathcal{F} \right\}$$</div><p></p>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#43b7d1; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Theorem.</strong> Consider a parameterized family of functions<br>
</p><div class="mathjax-exps">$$\mathcal{F} := \{ x\mapsto g(x,\theta): \theta\in\mathbb{R}^p \},$$</div><p></p>
<p>where <span class="mathjax-exps">$g:\mathbb{R}^m \times\mathbb{R}^p \to \{ 0,1 \}$</span>. Suppose <span class="mathjax-exps">$g$</span> can be computed using no more than <span class="mathjax-exps">$t$</span> operations of the following types:</p>
<ol>
<li>arithmetic (<span class="mathjax-exps">$+$</span>, <span class="mathjax-exps">$-$</span>, <span class="mathjax-exps">$\times$</span>, <span class="mathjax-exps">$\div$</span>)</li>
<li>comparisons of real numbers (<span class="mathjax-exps">$&gt;$</span>, <span class="mathjax-exps">$\geq$</span>, <span class="mathjax-exps">$=$</span>, <span class="mathjax-exps">$&lt;$</span>, <span class="mathjax-exps">$\leq$</span>, <span class="mathjax-exps">$\neq$</span>).</li>
</ol>
<p>Then <span class="mathjax-exps">$VC(\mathcal{F}) \leq 4p(t+2)$</span>.</p>
</div>
<br>
<hr>
<h3 id="quick-detour-bracketing-and-covering-numbers">Quick Detour: Bracketing and Covering Numbers. </h3>
<ul>
<li>Let <span class="mathjax-exps">$\mathcal{F}$</span> be a subset of an <span class="mathjax-exps">$L^(P)$</span> space.
<ul>
<li>
<p>For <span class="mathjax-exps">$r\geq 1$</span>, <span class="mathjax-exps">$L^r(P)$</span> is the space of functions <span class="mathjax-exps">$f: \mathcal{X} \to \mathbb{R}$</span> such that<br>
</p><div class="mathjax-exps">$$\Vert f \Vert_{L^r(P)} := \left[ \int |f(x)|^r dP(x) \right]^{1/r} &lt;\infty.$$</div><p></p>
<p>When <span class="mathjax-exps">$r=\infty$</span>, <span class="mathjax-exps">$L^r(P)$</span> consists of <span class="mathjax-exps">$f:\mathcal{X} \to \mathbb{R}$</span> such that<br>
</p><div class="mathjax-exps">$$\begin{align*}     \Vert f \Vert_{L^\infty(P)} &amp;:= {\text{ess}\sup}_P |f(x)| \\         &amp;:= \inf \left\{ a\in\mathbb{R}: P[|f(X)| &gt; a] = 0 \right\} \\         &amp;&lt; \infty. \end{align*}$$</div><p></p>
</li>
<li>
<p>For any <span class="mathjax-exps">$f:\mathcal{X} \to \mathbb{R}$</span>,<br>
</p><div class="mathjax-exps">$$\begin{align*}     \Vert f\Vert_{L^r(P)} \leq \Vert f \Vert_{L^q(P)} \hspace{2em} \text{if } r\leq q. \\ \\ \end{align*}$$</div><p></p>
</li>
</ul>
</li>
</ul>
<h4 id="varepsilon-bracketing-numbers"><span class="mathjax-exps">$\varepsilon$</span>-bracketing Numbers </h4>
<ul>
<li>
<p>Given two functions <span class="mathjax-exps">$\ell: \mathcal{X} \to \mathbb{R}$</span> and <span class="mathjax-exps">$u: \mathcal{X} \to \mathbb{R}$</span>, the <span class="mathjax-exps">$\textcolor{blue}{\text{bracket}}$</span> <span class="mathjax-exps">$[\ell,u]$</span> is defined as<br>
</p><div class="mathjax-exps">$$[\ell,u] := \left\{ f\in L^r(P): \ell \leq f \leq u \text{ pointwise} \right\}.$$</div><p></p>
</li>
<li>
<p>We call <span class="mathjax-exps">$[\ell,u]$</span> and <span class="mathjax-exps">$\textcolor{blue}{\varepsilon\text{-bracket}}$</span> if <span class="mathjax-exps">$\Vert u-\ell \Vert_{L^r(P)} \leq \varepsilon$</span>.</p>
</li>
<li>
<p>The <span class="mathjax-exps">$\textcolor{blue}{\varepsilon\text{-bracketing number}}$</span> <span class="mathjax-exps">$N_{[]}(\varepsilon, \mathcal{F}, L^r(P))$</span> of <span class="mathjax-exps">$\mathcal{F}$</span> is defined as the minimal number of <span class="mathjax-exps">$\varepsilon\text{-brackets}$</span> needed to cover <span class="mathjax-exps">$\mathcal{F}$</span>, that is<br>
</p><div class="mathjax-exps">$$\small{N_{[]}(\varepsilon, \mathcal{F}, L^r(P)) := \inf \left\{ m: \mathcal{F} \subseteq \bigcup_{j=1}^{m} [\ell_j, u_j] \text{ for a collection of } \varepsilon\text{-brackets } [\ell_j, u_j], j=1,\dots,m \right\}}$$</div><p></p>
</li>
</ul>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#42801f; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Definition.</strong> A function <span class="mathjax-exps">$d: S\times S \to [0,\infty)$</span> is called a <span class="mathjax-exps">$\textcolor{blue}{\text{pseudometric}}$</span> on <span class="mathjax-exps">$S$</span> if:</p>
<ol>
<li><span class="mathjax-exps">$d(x,x) =0$</span> for all <span class="mathjax-exps">$x\in S$</span>;</li>
<li><span class="mathjax-exps">$d(x,y) = d(y,x)$</span> for all <span class="mathjax-exps">$x,y\in S$</span>;</li>
<li><span class="mathjax-exps">$d(y,z) \leq d(y,x) + d(x,z)$</span> for all <span class="mathjax-exps">$x,y,z\in S$</span>.</li>
</ol>
<p>In contrast to a metric, <span class="mathjax-exps">$d(x,y)$</span> may be equal to 0 even if <span class="mathjax-exps">$x\neq y$</span>.</p>
<p>A <span class="mathjax-exps">$\textcolor{blue}{\text{pseudometric space}}$</span> <span class="mathjax-exps">$(S,d)$</span> is a pairing of a set <span class="mathjax-exps">$S$</span> with a pseudometric on that set, e.g. <span class="mathjax-exps">$d(f,g) := \Vert f - g \Vert_{L^r(P)}$</span>.</p>
</div>
<br>
<br>
<h4 id="covering-numbers">Covering Numbers </h4>
<ul>
<li>Let <span class="mathjax-exps">$(S,d)$</span> denote a pseudometric space and <span class="mathjax-exps">$T\subseteq S$</span>.</li>
<li>A set <span class="mathjax-exps">$T_1 \subseteq S$</span> in called an <span class="mathjax-exps">$\textcolor{blue}{\varepsilon\text{-cover}}$</span> of <span class="mathjax-exps">$T$</span> if, for all <span class="mathjax-exps">$\theta_1\in T_1$</span> such that <span class="mathjax-exps">$d(\theta, \theta_1) \leq \varepsilon$</span>.</li>
<li>The <span class="mathjax-exps">$\textcolor{blue}{\varepsilon\text{-covering number}}$</span> is defined as the size of the minimal <span class="mathjax-exps">$\varepsilon\text{-cover}$</span>:<br>
<div class="mathjax-exps">$$N(\varepsilon, T, d) = \inf \left\{ |T_1|: T_1 \text{ is an } \varepsilon\text{-cover of } T \right\}.$$</div></li>
</ul>
<p>Our goal will be to show that, if <span class="mathjax-exps">$\mathcal{F}$</span> is a VC class of boolean valued functions, then<br>
</p><div class="mathjax-exps">$$E\Vert P_n - P \Vert_\mathcal{F} = O \left( \sqrt{\frac{\log(n)}{n}} \right).$$</div><p></p>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#aa46e0; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Lemma (Finite class lemma).</strong> If <span class="mathjax-exps">$\mathcal{F}$</span> is a class <span class="mathjax-exps">$\mathcal{X} \to [-1,1]$</span> then<br>
</p><div class="mathjax-exps">$$\textcolor{blue}{E\Vert R_n \Vert_{\mathcal{F}} \leq \sqrt{\frac{2\log(2E[|F_{x_1^n}|])}{n}}}, \tag*{$\textcolor{blue}{(1)}$},$$</div><p></p>
<p>where <span class="mathjax-exps">$\mathcal{F}_{x_1^n} := \left\{ (f(X_1), f(X_2), \dots, f(X_n)): f\in\mathcal{F} \right\}$</span>d.<br>
<br></p>
<p><strong>Proof.</strong> In what follows, we'll establish that, for <span class="mathjax-exps">$P$</span>-almost all <span class="mathjax-exps">$x_1^n\in\mathcal{X}^n$</span>,<br>
</p><div class="mathjax-exps">$$E \left[ \Vert R_n \Vert_{\mathcal{F}} \mid X_1^n = x_1^n \right] \leq \sqrt{\frac{2\log(2|F_{x_1^n}|)}{n}}. \tag*{$\textcolor{orange}{(2)}$}.$$</div><p></p>
<p>This will yield <span class="mathjax-exps">$\textcolor{blue}{(1)}$</span> after integrating both sides against <span class="mathjax-exps">$P^n$</span> and applying Jensen's inequality.</p>
<p>We now establish <span class="mathjax-exps">$\textcolor{orange}{(2)}$</span>. For <span class="mathjax-exps">$z\in[-1,1]^n$</span>, we'll let <span class="mathjax-exps">$\langle \varepsilon, z \rangle = \sum_{j=1}^{n} \varepsilon_j z_j$</span>. Let<br>
</p><div class="mathjax-exps">$$\mathcal{Z}_{x_1^n} = \mathcal{F}_{x_1^n} \;\bigcup \underbrace{(-\mathcal{F}_{x_1^n})}_{\textcolor{purple}{:= \left\{ -z: z\in\mathcal{F}_{x_1^n}\right\}}}.$$</div><p></p>
<p>For any <span class="mathjax-exps">$\lambda&gt;0$</span>,<br>
</p><div class="mathjax-exps">$$\begin{align*}     \exp \left\{ \lambda E \left[ \sup_{f\in\mathcal{F}} | \langle \varepsilon, f(x_1^n) \rangle \right] \right\} &amp;= \exp \left\{ \lambda E \left[ \sup_{z\in\mathcal{Z}_{x_1^n}} \langle \varepsilon, z \rangle \right] \right\} \tag*{$\textcolor{purple}{|a| = \max\{ -a,a \}}$} \\         &amp;\leq E \left[ \exp \left\{ \lambda \sup_{z\in\mathcal{Z}_{x_1^n}} \langle \varepsilon, z \rangle \right\} \right] \tag*{$\textcolor{purple}{\text{Jensen's inequality}}$} \\         &amp;= E \left[ \sup_{z\in\mathcal{Z}_{x_1^n}} \exp \left\{ \lambda \langle \varepsilon, z \rangle \right\} \right] \tag*{$\textcolor{purple}{\text{monotonicity of } \exp\{ \lambda \, \cdot \}}$} \\         &amp;\leq \sum_{z\in\mathcal{Z}_{x_1^n}} \underbrace{E \left[ \exp \{ \lambda \langle \varepsilon, z \rangle \} \right]}_{\textcolor{purple}{\text{MGF of } \langle \varepsilon, z \rangle}!} \end{align*}$$</div><p></p>
<p>Because <span class="mathjax-exps">$\varepsilon_i z_i$</span> is a random variable bounded in <span class="mathjax-exps">$[-1,1]$</span>, it is sub-Gaussian with parameter <span class="mathjax-exps">$1$</span>. Hence, the iid sum <span class="mathjax-exps">$\langle \varepsilon, z \rangle$</span> is sub-Gaussian with parameter <span class="mathjax-exps">$n$</span>.</p>
<p>Plugging this into the above, we have<br>
</p><div class="mathjax-exps">$$\begin{align*}     \exp \left\{ \lambda E \left[ \sup_{f\in\mathcal{F}} \left| \langle \varepsilon, f(x_1^n) \rangle \right| \right] \right\} &amp;\leq \sum_{z\in\mathcal{Z}_{x_1^n}} \exp \left( \frac{\lambda^2 n}{2} \right) \\         &amp;= \left| \mathcal{Z}_{x_1^n} \right| \exp \left( \frac{\lambda^2 n}{2} \right). \end{align*}$$</div><p></p>
<p>Hence,<br>
</p><div class="mathjax-exps">$$E \left[ \sup_{f\in\mathcal{F}} \left| \langle \varepsilon, f(x_1^n) \rangle \right| \right] \leq \frac{\log \left| \mathcal{Z}_{x_1^n} \right|}{\lambda} + \frac{\lambda_n}{2}.$$</div><p></p>
<p>Plugging in <span class="mathjax-exps">$\lambda = \sqrt{\frac{2\log \left| \mathcal{Z}_{x_1^n}\right|}{n}}$</span> and using that <span class="mathjax-exps">$\left| \mathcal{Z}_{x_1^n} \right| \leq 2 \left| \mathcal{F}_{x_1^n} \right|$</span> gives <span class="mathjax-exps">$\textcolor{orange}{(2)}$</span>.</p>
</div>
<br>
<p>By the above lemma and the definition of the growth function, when <span class="mathjax-exps">$\mathcal{F}$</span> consists of boolean-valued functions,<br>
</p><div class="mathjax-exps">$$\begin{align*}     E\Vert P_n - P \Vert_\mathcal{F} &amp;\leq 2 E \Vert R_n \Vert_{\mathcal{F}} \\         &amp;\leq 2 \sqrt{ \frac{2\log (2 E[|\mathcal{F}_{x_1^n}])}{n}} \\         &amp;\leq 2 \sqrt{\frac{2\log\left(2\sup_{x_1^n} |\mathcal{F}_{x_1^n}|\right)}{n}} \hspace{2em} \textcolor{blue}{\text{where } \Pi_{\mathcal{F}}(n) = \sup_{x_1^n} |\mathcal{F}_{x_1^n}|} \\         &amp;= 2 \sqrt{\frac{2\log(2\Pi_\mathcal{F}(n))}{n}}. \tag*{$\textcolor{red}{(3)}$} \end{align*}$$</div><p></p>
<p>If <span class="mathjax-exps">$\Pi_\mathcal{F}(n) = 2^n$</span>, this bound is trivial. But if <span class="mathjax-exps">$n&gt;VC(\mathcal{F})$</span>, then <span class="mathjax-exps">$\Pi_\mathcal{F}(n)&lt;2^n$</span>. In fact, it turns out that <span class="mathjax-exps">$\Pi_{\mathcal{F}}(n)$</span> will be <strong>much</strong> smaller than <span class="mathjax-exps">$2^n$</span>!</p>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#aa46e0; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Lemma (Sauer's lemma).</strong> Fix <span class="mathjax-exps">$d\geq VC(\mathcal{F})$</span>. It holds that<br>
</p><div class="mathjax-exps">$$\Pi_\mathcal{F}(n) \leq \sum_{k=0}^{\min\{ n,d \}} {n \choose k}.$$</div><p></p>
<p>Hence,<br>
</p><div class="mathjax-exps">$$\Pi_\mathcal{F}(n) \leq \begin{cases}     2^n, &amp; n\leq d \\     \left( \frac{e}{d} \right)^d n^d, &amp; n&gt;d. \end{cases}$$</div><p></p>
</div>
<br>
<p>By <span class="mathjax-exps">$\textcolor{red}{(3)}$</span>, the above implies that, if <span class="mathjax-exps">$n&gt;d\geq VC(\mathcal{F})$</span> and <span class="mathjax-exps">$\mathcal{F}$</span> consists of boolean-valued functions, then<br>
</p><div class="mathjax-exps">$$\begin{align*}     E \Vert P_n - P\Vert_\mathcal{F} &amp;\leq 2 \sqrt{\frac{2\log(2) + 2d\log\left( \frac{e}{d} \cdot n \right)}{n}} \\         &amp;= O \left( \sqrt{\frac{\log(n)}{n}} \right). \end{align*}$$</div><p></p>
<p><strong>Some things to note:</strong></p>
<ul>
<li>The above is helpful for deriving regret bounds for ERMs when the loss is boolean-valued.</li>
<li><u>But:</u> for general <span class="mathjax-exps">$\mathbb{R}$</span>-valued losses, it's not helpful.</li>
</ul>
<p>(abrupt transition back to covering numbers because we are going out of sequence)</p>
<p></p>
<br>
<p style="border-spacing:0px; page-break-before: always">
</p><h2 id="uchapter-4-part-2u"><u>Chapter 4: Part 2.</u> </h2>
<p><big><strong>Chapter 4, Part 2 Table of Contents</strong></big></p>
<ul>
<li><a href="chapter4\part2\chapter4-part2.md#back-to-bracketing-and-covering-numbers">Back to Bracketing and Covering Numbers</a></li>
<li><a href="chapter4\part2\chapter4-part2.md#stochastic-process">Stochastic Process</a></li>
<li><a href="chapter4\part2\chapter4-part2.md#motivation-for-dudleys-entropy-integral-bound">Motivation for Dudley's entropy integral bound.</a></li>
</ul>
<p><strong>Other Sections:</strong> <a href="#uchapter-1u-minimaxity">Chapter 1</a>, <a href="#uchapter-2u-kernel-density-estimation">Chapter 2</a>, <a href="#uchapter-3u-concentration-inequality">Chapter 3</a>, <a href="#uchapter-4-part-1u">Chapter 4 - Part 1</a></p>
<br>
<p></p><h3 id="back-to-bracketing-and-covering-numbers">Back to Bracketing and Covering Numbers </h3>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#0000ff; padding:0.75em; padding-bottom: 0em;">
<p><big><strong>Example:</strong> <span class="mathjax-exps">$N(\varepsilon, [0,1]^2, \ell^\infty) = O\left( \tfrac{1}{\varepsilon^2} \right)$</span>. </big></p>
<p><img src="chapter4\figures\ex-1.jpg" alt="Example 1."></p>
</div>
<br>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#0000ff; padding:0.75em; padding-bottom: 0em;">
<p><big><strong>Example:</strong> Functions that are Lipschitz in their indexing parameter.</big></p>
<p>Let <span class="mathjax-exps">$f: \mathcal{X} \times B \to \mathbb{R}$</span> be some function and<br>
</p><div class="mathjax-exps">$$\left\{ x\mapsto f(x,\beta): \beta\in B \right\}.$$</div><p></p>
<p>For example, <span class="mathjax-exps">$f(x,\beta) = [y-\beta^\top w]^2$</span> with <span class="mathjax-exps">$x=(w,y)$</span> and <span class="mathjax-exps">$B$</span> a bounded set of <span class="mathjax-exps">$\mathbb{R}^p$</span>.</p>
<p>Suppose that <span class="mathjax-exps">$f$</span> satisfies the following Lipschitz condition:</p>
<ul>
<p>There exists <span class="mathjax-exps">$L&gt;0$</span> such that, for all <span class="mathjax-exps">$\beta_1,\beta_2 \in B$</span>,<br>
</p><div class="mathjax-exps">$$\Vert f(\cdot, \beta_1) - f(\cdot, \beta_2) \Vert_\mathcal{F} \leq L \Vert \beta_1 - \beta_2 \Vert_B,$$</div><p></p>
<p>where <span class="mathjax-exps">$\Vert \cdot \Vert_\mathcal{F}$</span> is a norm on <span class="mathjax-exps">$\mathcal{F}$</span> and <span class="mathjax-exps">$\Vert \cdot \Vert_B$</span> is a norm on <span class="mathjax-exps">$B$</span>.</p>
</ul>
<p><strong>Claim:</strong> <span class="mathjax-exps">$N(\varepsilon, \mathcal{F}, \Vert \cdot \Vert_{\mathcal{F}}) \leq N\left( \frac{\varepsilon}{L}, B, \Vert \cdot \Vert_B \right)$</span>.</p>
<br>
<p><strong>Proof.</strong> Let <span class="mathjax-exps">$\{ \beta_j \}_{j=1}^{n}$</span> denote a minimal <span class="mathjax-exps">$\frac{\varepsilon}{L}$</span> cover of <span class="mathjax-exps">$B$</span>, so that <span class="mathjax-exps">$n=N\left( \frac{\varepsilon}{L}, B, \Vert \cdot \Vert_B \right)$</span>. We'll show that <span class="mathjax-exps">$\{ f(\cdot, \beta_j) \}_{j=1}^{n}$</span> is an <span class="mathjax-exps">$\varepsilon$</span>-cover of <span class="mathjax-exps">$\mathcal{F}$</span>.</p>
<p>Fix <span class="mathjax-exps">$g\in\mathcal{F}$</span> and let <span class="mathjax-exps">$\beta\in B$</span> be such that <span class="mathjax-exps">$g(\cdot) = f(\cdot,\beta)$</span>. Let <span class="mathjax-exps">$\beta_j$</span> be such that <span class="mathjax-exps">$\Vert \beta_j - \beta \Vert_B \leq \frac{\varepsilon}{L}$</span>. Combining this with the <span class="mathjax-exps">$\textcolor{blue}{\text{Lipschitz-condition}}$</span> gives<br>
</p><div class="mathjax-exps">$$\begin{align*}     \Vert f(\cdot, \beta) - f(\cdot, \beta_j) \Vert_{\mathcal{F}} &amp;\leq L \Vert \beta-\beta_j \Vert_B \\         &amp;\leq L \cdot \frac{\varepsilon}{L} \\         &amp;= \varepsilon. \end{align*}$$</div><p></p>
</div>
<br>
<p><strong>Recall:</strong> <span class="mathjax-exps">$\Vert f \Vert_\infty := \sup_x |f(x)|$</span>.</p>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#43b7d1; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Theorem.</strong> Let <span class="mathjax-exps">$F \subset L^r(P)$</span>, <span class="mathjax-exps">$r\in[1,\infty]$</span>. For <span class="mathjax-exps">$\varepsilon&gt;0$</span>,<br>
</p><div class="mathjax-exps">$$N_{[]}(2\varepsilon, \mathcal{F}, L^r(P)) \leq N(\varepsilon, \mathcal{F}, \Vert \cdot \Vert_{\infty}).$$</div><p></p>
<br>
<p><strong>Proof.</strong> Let <span class="mathjax-exps">$\{ f_j \}_{j=1}^{n}$</span> denote a minimal <span class="mathjax-exps">$\varepsilon$</span>-cover of <span class="mathjax-exps">$\mathcal{F}$</span>, so that <span class="mathjax-exps">$n=N(\varepsilon,\mathcal{F}, \Vert \cdot \Vert_\infty)$</span>. For each <span class="mathjax-exps">$j\in\{ 1,2,\dots,n \}$</span>, define the <span class="mathjax-exps">$2\varepsilon$</span>-bracket<br>
</p><div class="mathjax-exps">$$[f_j - \varepsilon, f_j + \varepsilon].$$</div><p></p>
<p><strong>Why is this a <span class="mathjax-exps">$2\varepsilon$</span>-bracket?</strong><br>
</p><div class="mathjax-exps">$$\Vert f_j + \varepsilon - (f_j - \varepsilon) \Vert_{L^r(P)} = \Vert 2\varepsilon \Vert_{L^(P)} = 2\varepsilon.$$</div><p></p>
<p>Because <span class="mathjax-exps">$\{ f_j \}_{j=1}^{n}$</span> is an <span class="mathjax-exps">$\varepsilon$</span>-cover of <span class="mathjax-exps">$\mathcal{F}$</span> relative to <span class="mathjax-exps">$\Vert\cdot \Vert_\infty$</span>, it holds that<br>
</p><div class="mathjax-exps">$$\mathcal{F}\subseteq \bigcup_{j=1}^{n} [f_j-\varepsilon, f_j+\varepsilon].$$</div><p></p>
<p>Hence, we've exhibited a collection of <span class="mathjax-exps">$n$</span> <span class="mathjax-exps">$2\varepsilon$</span>-brackets that covers <span class="mathjax-exps">$\mathcal{F}$</span>.</p>
</div>
<br>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#000000; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Reminder:</strong> To control the regret of an empirical risk minimizer, it's enough to control <span class="mathjax-exps">$E\Vert P_n-P \Vert_\mathcal{F}\leq 2E\Vert R_n\Vert_\mathcal{F}$</span>.</p>
</div>
<br>
<br>
<h3 id="stochastic-process">Stochastic Process </h3>
<ul>
<li>A <span class="mathjax-exps">$\textcolor{blue}{\text{stochastic process}}$</span> <span class="mathjax-exps">$\{ X_\theta:\theta\in T \}$</span> is a collection of random variables.</li>
<li>A process is called <span class="mathjax-exps">$\textcolor{blue}{\text{mean zero}}$</span> if <span class="mathjax-exps">$E[X_\theta] = 0$</span> for all <span class="mathjax-exps">$\theta\in T$</span>.</li>
<li>We'll find it useful to be able to say that <span class="mathjax-exps">$\textcolor{purple}{X_{\theta_1} \text{ is ``close" to } X_{\theta_2}}$</span> is <span class="mathjax-exps">$\textcolor{purple}{{\theta_1} \text{ is ``close" to } {\theta_2}}$</span> in some appropriate sense.</li>
<li>To formalize this, we define the notion of a <span class="mathjax-exps">$\textcolor{blue}{\text{sub-Gaussian process}}$</span>.</li>
</ul>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#42801f; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Definition.</strong> A mean zero stochastic process is called <span class="mathjax-exps">$\textcolor{blue}{\text{sub-Gaussian with respect to } d}$</span>, with <span class="mathjax-exps">$d$</span> a pseudometric on <span class="mathjax-exps">$T$</span>, if, for all <span class="mathjax-exps">$\theta,\theta'\in T$</span> and <span class="mathjax-exps">$\lambda\in\mathbb{R}$</span>,<br>
</p><div class="mathjax-exps">$$\log E \left[ \exp \left\{ \lambda (X_\theta - X_{\theta'}) \right\} \right] \leq \frac{\lambda^2 d(\theta, \theta')^2}{2}.$$</div><p></p>
<br>
<p><strong>Equivalent definition:</strong> A mean zero stochastic process is called sub-Gaussian with respect to <span class="mathjax-exps">$d$</span> if, for all <span class="mathjax-exps">$\theta,\theta'\in T$</span>, <span class="mathjax-exps">$X_\theta - X_{\theta'}$</span> is sub-Gaussian with parameter <span class="mathjax-exps">$d(\theta,\theta')^2$</span>.</p>
</div>
<br>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#0000ff; padding:0.75em; padding-bottom: 0em;">
<p><big><strong>Example:</strong> Canonical Rademacher Process.</big></p>
<ul>
<li>
<p>Let <span class="mathjax-exps">$S=\mathbb{R}^n$</span> and <span class="mathjax-exps">$d$</span> denote the Euclidean metric on <span class="mathjax-exps">$S$</span>.</p>
</li>
<li>
<p>Let <span class="mathjax-exps">$T\subseteq S$</span> denote the index set.</p>
</li>
<li>
<p>Let <span class="mathjax-exps">$r_1,r_2,\dots,r_n$</span> denote iid Rademacher random variables.</p>
</li>
<li>
<p>The <span class="mathjax-exps">$\textcolor{blue}{\text{canonical Rademacher process}}$</span> <span class="mathjax-exps">$\{ X_\theta: \theta\in T \}$</span> is define so that<br>
</p><div class="mathjax-exps">$$X_\theta = \sum_{j=1}^{n} \theta_j r_j = \langle \theta, r \rangle,$$</div><p></p>
<p>where <span class="mathjax-exps">$r=(r_1,\dots,r_n)$</span>.</p>
</li>
<li>
<p>Using results from Chapter 3, you can show <span class="mathjax-exps">$\{ X_\theta: \theta\in T \}$</span> is sub-Gaussian with respect to the Euclidean metric <span class="mathjax-exps">$d$</span>.</p>
</li>
<li>
<p>Why is this process relevant to us?</p>
<ul>
<li>When <span class="mathjax-exps">$T=\mathcal{F}_{x_1^n} \bigcup (- \mathcal{F}_{z_1^n})$</span> (where <span class="mathjax-exps">$z_1^n$</span> is a generic realization of <span class="mathjax-exps">$Z_1,Z_2,\dots,Z_n \stackrel{iid}{\sim} P$</span>), we'll show that<br>
<div class="mathjax-exps">$$E [\Vert R_n \Vert_{\mathcal{F}} \mid Z_1^n = z_1^n] = \frac{1}{n} E \left[ \sup_{\theta\in T} X_\theta \right].$$</div></li>
</ul>
</li>
</ul>
</div>
<br>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#aa46e0; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Lemma (Finite class lemma).</strong> If <span class="mathjax-exps">$\{ X_\theta: \theta\in T \}$</span> is sub-Gaussian with respect to <span class="mathjax-exps">$d$</span> and <span class="mathjax-exps">$A\subseteq T\times T$</span>, then<br>
</p><div class="mathjax-exps">$$E \left[ \max_{(\theta,\theta') \in A} (X_\theta - X_{\theta'}) \right] \leq \sqrt{2\log |A|} \underbrace{{\max_{(\theta,\theta')\in A} d(\theta, \theta')}}_{\textcolor{purple}{\text{``the diameter of } A "}}$$</div><p></p>
</div>
<br>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#43b7d1; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Theorem (one-step discretization bound).</strong> Let <span class="mathjax-exps">$\{ X_\theta: \theta\in T \}$</span> denote a mean zero sub-Gaussian process with respect to <span class="mathjax-exps">$d$</span>. Let <span class="mathjax-exps">$D:= \sup_{\theta,\theta'\in T} d(\theta, \theta')$</span> denote the diameter of <span class="mathjax-exps">$T$</span>. For any <span class="mathjax-exps">$\varepsilon&gt;0$</span>,<br>
</p><div class="mathjax-exps">$$E \left[ \sup_{\theta\in T} X_\theta \right] \leq 2 E \left[ \textcolor{pink}{\sup_{(\theta,\theta')\in T: d(\theta,\theta')\leq \varepsilon} (X_\theta - X_{\theta'})} \right] + \textcolor{blue}{2D \sqrt{\log N(\varepsilon, T, d)}}.$$</div><p></p>
<p>Note that as <span class="mathjax-exps">$\textcolor{blue}{\varepsilon \downarrow 0}$</span>, we have<br>
</p><div class="mathjax-exps">$$\displaystyle \textcolor{pink}{\sup_{(\theta,\theta')\in T: d(\theta,\theta')\leq \varepsilon} (X_\theta - X_{\theta'})} \;\;\;{\large \searrow}$$</div><p></p>
<p>and<br>
</p><div class="mathjax-exps">$$\displaystyle \textcolor{blue}{2D \sqrt{\log N(\varepsilon, T, d)}}\;\;\;{\nearrow}$$</div><p></p>
<br>
<p><strong>Proof.</strong> For any <span class="mathjax-exps">$\theta'\in T$</span>, the fact that <span class="mathjax-exps">$\{ X_\theta: \theta\in T \}$</span> is mean zero implies that<br>
</p><div class="mathjax-exps">$$\begin{align*}     E \left[ \sup_{\theta\in T} X_\theta \right] &amp;= E \left[ \sup_{\theta\in T} (X_\theta - X_{\theta'}) \right] \\         &amp;\leq E \left[ \sup_{\theta,\theta'\in T} (\textcolor{blue}{X_\theta - X_{\theta'} }) \right]. \end{align*}$$</div><p></p>
<p>Let <span class="mathjax-exps">$T_1$</span> denote a minimal <span class="mathjax-exps">$\varepsilon$</span>-cover of <span class="mathjax-exps">$T$</span>. For any <span class="mathjax-exps">$\theta,\theta'\in T$</span>, there exist <span class="mathjax-exps">$\theta_1, \theta_1' \in T_1$</span> such that <span class="mathjax-exps">$d(\theta,\theta_1) \leq \varepsilon$</span> and <span class="mathjax-exps">$d(\theta', \theta_1') \leq \varepsilon$</span>. Observe that<br>
</p><div class="mathjax-exps">$$\begin{align*}     \textcolor{blue} {X_\theta - X_{\theta'}} &amp;= (X_\theta - X_{\theta_1}) - (X_{\theta'} - X_{\theta_1'}) + (X_{\theta_1} - X_{\theta_1'}) \\         &amp;\leq 2 \sup_{\theta_2,\theta_3\in T: d(\theta_2, \theta_3) \leq \varepsilon} (X_{\theta_2} - X_{\theta_3}) + \max_{(\theta_4, \theta_5)\in T_1\times T_1} (X_{\theta_4} - X_{\theta_5}). \end{align*}$$</div><p></p>
<p>Combining the above two displays, we have,<br>
</p><div class="mathjax-exps">$$E \left[ \sup_{\theta\in T} X_\theta \right] \leq 2 E \left[ \sup_{\theta_2, \theta_3\in T: d(\theta_2,\theta_3) \leq \varepsilon} (X_{\theta_2} - X_{\theta_3}) \right] + \underbrace{E \left[ \max_{(\theta_4, \theta_5)\in T_1\times T_1} (X_{\theta_4} - X_{\theta_5}) \right]}_{\textcolor{red}{(\bigstar)}}.$$</div><p></p>
<p>By the finite class lemma,<br>
</p><div class="mathjax-exps">$$\begin{align*}     \textcolor{red}{(\bigstar)} &amp;\leq \left( 2 \log | T_1\times T_1| \right)^{1/2} D \\         &amp;= 2 \left( \log |T_1| \right)^{1/2} D \\         &amp;= 2 \sqrt{ \log N(\varepsilon, T, d)} D. \end{align*}$$</div><p></p>
</div>
<br>
<p>We'll now use the one-step discretization bound to establish the following result:</p>
<div style="border-spacing:0px; border-width:1.5px; border-style:solid; border-color:#43b7d1; padding: 0.75em; padding-bottom:0em; page-break-inside:avoid;">
<p><strong>Theorem.</strong> If <span class="mathjax-exps">$\mathcal{F}$</span> is a class of functions whose ranges are bounded in <span class="mathjax-exps">$[-M, M]$</span>, for for any <span class="mathjax-exps">$\delta&gt;0$</span>, we have<br>
</p><div class="mathjax-exps">$$E \Vert R_n \Vert_{\mathcal{F}} \leq 2\delta + 4Mn^{-1/2} \sup_Q \sqrt{ \log[2N(\delta, \mathcal{F}, L^2(Q))]},$$</div><p></p>
<p>where the supremum  is over all finitely supported distributions whose support is contained in that of <span class="mathjax-exps">$P$</span>.</p>
<p><strong>Proof.</strong></p>
<ul>
<li>
<p><span style="color:blue;">  <b>Step 1:</b> Relating the Rademacher complexity of <span class="mathjax-exps">$\mathcal{F}$</span> to the expected supremum of a sub-Gaussian process. </span><br>
<br></p>
<p><u>Recall:</u><br>
</p><div class="mathjax-exps">$$\mathcal{F}_{z_1^n} := \left\{ (f(z_1), f(z_2), \dots, f(z_n)): f\in\mathcal{F} \right\}.$$</div><p></p>
<p>Let <span class="mathjax-exps">$T:= \mathcal{F}_{z_1^n} \cup (-\mathcal{F}_{z_1^n})$</span>. We consider the canonical Rademacher process<br>
</p><div class="mathjax-exps">$$\left\{ X_\theta := \sum_{j=1}^{n} \theta_j r_j : \theta\in T\subseteq \mathbb{R}^n \right\},$$</div><p></p>
<p>noting that <span class="mathjax-exps">$\sum_{j=1}^{n} \theta_j r_j =: \langle \theta, r \rangle$</span>.<br>
<br></p>
<p>The empirical Rademacher complexity rewrites as follows:<br>
</p><div class="mathjax-exps">$$\begin{align*}   E \left[ \Vert R_n \Vert_{\mathcal{F}} \mid Z_1^n = z_1^n \right] &amp;= E \left[ \left. \sup_{f\in\mathcal{F}} \left| \frac{1}{n} \sum_{j=1}^{n} f(Z_j) r_j  \right| \, \right| Z_1^n = z_1^n \right] \\     &amp;= \frac{1}{n} E \left[ \left. \sup_{f\in\mathcal{F}} \left| \langle f(z_1^n), r \rangle \right| \right| Z_1^n = z_1^n \right] \\     &amp;= \frac{1}{n} E \left[ \left. \sup_{f\in\mathcal{F} \cup (-\mathcal{F})} \langle f(z_1^n), r \rangle \right| Z_1^n = z_1^n \right] \tag*{\small$\langle f(z_1^n), r \rangle = X_\theta$ with $\theta=f(z_1^n)$\normalsize}\\     &amp;= \frac{1}{n} E \left[ \sup_{\theta\in T} X_\theta \right]. \end{align*}$$</div><p></p>
</li>
<li>
<p><span style="color:blue;">  <b>Step 2:</b> Applying the one-step discretization bound to <span class="mathjax-exps">$\displaystyle E\left[ \sup_{\theta\in T} X_\theta \right]$</span>. </span><br>
<br></p>
<p>By that bound, for any <span class="mathjax-exps">$\varepsilon&gt;0$</span>,<br>
</p><div class="mathjax-exps">$$\textcolor{red}{\boxed{E\left[ \sup_{\theta\in T} X_\theta \right] \leq  2 E \left[ \sup_{\theta,\theta'\in T: d(\theta,\theta')\leq \varepsilon} (X_\theta - X_{\theta'}) \right] + 2D \sqrt{\log N(\varepsilon, T, d)}}},$$</div><p></p>
<p>where <span class="mathjax-exps">$d$</span> is the Euclidean metric in this case.<br>
<br></p>
<p>In the remainder of our argument, we study the two terms on the right hand side.<br>
<br></p>
<p>For the first,<br>
</p><div class="mathjax-exps">$$\begin{align*}     E\left[ \sup_{\theta, \theta' \in T: d(\theta,\theta')\leq \varepsilon} (X_\theta - X_{\theta'}) \right] &amp;= E \left[ \sup_{\theta, \theta' \in T: d(\theta,\theta')\leq \varepsilon} \langle \theta-\theta', r \rangle \right] \\         &amp;\leq E \left[ \sup_{\theta,\theta'\in\mathbb{R}^n: d(\theta,\theta')\leq \varepsilon} \langle \theta-\theta', r \rangle \right] \\         &amp;= E \left[ \langle v,r \rangle |_{v=\varepsilon r / \Vert r \Vert} \right] \\         &amp;= \varepsilon E \left[ \Vert r \Vert_2 \right] \\         &amp;= \varepsilon n^{1/2}. \end{align*}$$</div><p></p>
<p>Pluggin this into <span style="display:inline-block;border-spacing:0px; border-width:1.5px;border-style:solid;border-color:#FC0000"> &nbsp;our earlier bound&nbsp;</span>,<br>
</p><div class="mathjax-exps">$$E\left[ \sup_{\theta\in T} X_\theta \right] \leq 2\varepsilon n^{1/2} + 2D \sqrt{\log N(\varepsilon, T, \Vert \cdot \Vert_2)}.$$</div><p></p>
<p>Recalling step 1, we've shown<br>
</p><div class="mathjax-exps">$$\textcolor{#fc9300}{\boxed{E\left[ \Vert R_n \Vert_\mathcal{F} | Z_1^n = z_1^n \right] \leq 2 \varepsilon n^{-1/2} + 2n^{-1} D\sqrt{\log N(\varepsilon, T, \Vert \cdot \Vert_2)}}}.$$</div><p></p>
</li>
<li>
<p><span style="color:blue;">  <b>Step 3:</b> Find a bound in terms of the covering number of <span class="mathjax-exps">$\mathcal{F}$</span>. </span><br>
<br></p>
<p>To do this, we relate the covering numbers of <span class="mathjax-exps">$\mathcal{F}$</span> to those of <span class="mathjax-exps">$T:= \mathcal{F}_{z_1^n} \cup (-\mathcal{F}_{z_1^n})$</span>.</p>
<ul>
<li>
<p>Fix <span class="mathjax-exps">$\theta_1,\theta_2\in T$</span>. There exist <span class="mathjax-exps">$f_1,f_2\in \mathcal{F}\cup (-\mathcal{F})$</span> such that <span class="mathjax-exps">$\theta_1 = (f_1(z_1), \dots, f_1(z_n))$</span> and <span class="mathjax-exps">$\theta_2 = (f_2(z_1), \dots, f_2(z_n))$</span>. Hence,<br>
</p><div class="mathjax-exps">$$\begin{align*}   \Vert \theta_1 - \theta_2 \Vert &amp;= \sqrt{\sum_{i=1}^n [\theta_{1i} - \theta_{2i}]^2} \\       &amp;= \sqrt{\sum_{i=1}^{n} [f_1(z_i) - f_2(z_i)]^2} \\       &amp;= n^{1/2} \sqrt{\frac{1}{n} \sum_{i=1}^n  [f_1(z_i) - f_2(z_i)]^2} \\       &amp;= n^{1/2} \Vert f_1 - f_2 \Vert_{L^2(P_n)}, \end{align*}$$</div><p></p>
<p>where <span class="mathjax-exps">$P_n$</span> denotes the empirical distribution when <span class="mathjax-exps">$Z_1^n = z_1^n$</span>.<br>
<br></p>
<p>Using the above,<br>
</p><div class="mathjax-exps">$$N ( \varepsilon, T, \Vert \cdot \Vert_2) = N\left(\varepsilon n^{-1/2}, \mathcal{F} \cup (-\mathcal{F}), L^2(P_n) \right).$$</div><p></p>
</li>
<li>
<p>Plugging this into <span style="display:inline-block;border-spacing:0px; border-width:1.5px;border-style:solid;border-color:#fc9300"> &nbsp;step 2&nbsp;</span>,<br>
</p><div class="mathjax-exps">$$\begin{align*}   E \left[ \Vert R_n \Vert_{\mathcal{F}} | Z_1^n=z_1^n \right] &amp;\leq 2 n^{-1/2} \varepsilon + 2 D n^{-1} \sqrt{\log N\left( \varepsilon n^{-1/2}, \mathcal{F}\cup (-\mathcal{F}), L^2(P_n) \right)} \\       &amp;\leq 2 n^{-1/2} \varepsilon + 2 D n^{-1} \sup_{Q}\sqrt{\log N\left( \varepsilon n^{-1/2}, \mathcal{F}\cup (-\mathcal{F}), L^2(Q) \right)} \\       &amp;= 2\delta + 2Dn^{-1} \sup_Q \sqrt{\log N\left( \delta, \mathcal{F}\cup (-\mathcal{F}), L^2(P_n) \right)}, \end{align*}$$</div><p></p>
<p>where <span class="mathjax-exps">$\delta=\varepsilon/\sqrt{n}$</span>. Above, the supremum is over all finitely supported distributions whose support is contained in that of <span class="mathjax-exps">$\mathcal{P}$</span>.<br>
<br></p>
<p>We now make the dependence of our diameter <span class="mathjax-exps">$D$</span> on <span class="mathjax-exps">$z_1^n$</span> explicit in the notation by writing <span class="mathjax-exps">$D_{z_1^n}$</span>.<br>
<br></p>
<p>Using that functions in <span class="mathjax-exps">$\mathcal{F}$</span> are bounded in <span class="mathjax-exps">$[-M, M]$</span>,<br>
</p><div class="mathjax-exps">$$\begin{align*}   D_{z_1^n} &amp;:= \sup_{\theta_1,\theta_2\in \mathcal{F}_{z_1^n} \cup (-\mathcal{F}_{z_1^n})} \Vert \theta_1 - \theta_2 \Vert_2 \\       &amp;= \sup_{f_1,f_2\in \mathcal{F}\cup (-\mathcal{F})} \Vert f_1 - f_2 \Vert_{L^2(P_n)} n^{1/2} \\       &amp;\leq 2Mn^{1/2}. \end{align*}$$</div><p></p>
</li>
<li>
<p>Lastly, we note that<br>
</p><div class="mathjax-exps">$$N(\delta, \mathcal{F}\cup (-\mathcal{F}), L^2(Q)) \leq 2N(\delta, \mathcal{F}, L^2(Q)).$$</div><p></p>
<p>Putting everything together, we have<br>
</p><div class="mathjax-exps">$$E \left[ \Vert R_n \Vert_{\mathcal{F}} | Z_1^n = z_1^n \right] \leq 2\delta + 4Mn^{-1/2} \sup_Q \sqrt{\log[2 N(\delta, \mathcal{F}, L^2(Q))]}.$$</div><p></p>
<p>Integrating both sides against <span class="mathjax-exps">$P^n$</span> gives our desired bound on <span class="mathjax-exps">$E\left[ \Vert R_n \Vert_{\mathcal{F}} \right]$</span>.</p>
</li>
</ul>
</li>
</ul>
</div>
<br>
<br>
<h3 id="motivation-for-dudleys-entropy-integral-bound">Motivation for Dudley's entropy integral bound. </h3>
<p>Recall that the one-step discretization bound show that<br>
</p><div class="mathjax-exps">$$E\left[ \Vert R_n \Vert_{\mathcal{F}} \right] \leq 2\delta + 4Mn^{-1/2} \sup_Q \sqrt{\log[2 N(\delta, \mathcal{F}, L^2(Q))]}.$$</div><p></p>
<p>A key observation made by Dudley is that, if <span class="mathjax-exps">$\mathcal{F}$</span> isn't "too big" in an appropriate sense, the latter term above can be replaced by a term that doesn't go to <span class="mathjax-exps">$+\infty$</span> as <span class="mathjax-exps">$\delta \downarrow 0$</span>.</p>
<p></p>

      </div>
      
      
    
    
    
    
    
    
  
    </body></html>
